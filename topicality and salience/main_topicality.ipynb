{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load utils_topicality.py\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import ipdb\n",
    "\n",
    "\n",
    "data_dir = \"../datasets/semeval_2010_t1_eng/data\"\n",
    "#libs_dir = \"hdp\"\n",
    "#sys.path.append(libs_dir)\n",
    "from hdp.ugs import *\n",
    "\n",
    "numpy.seterr(divide='ignore')\n",
    "\n",
    "split_re = re.compile(r'([()|])')\n",
    "INF = 1e10\n",
    "\n",
    "class Event:\n",
    "\tdef __init__(self,_phrase='',_pos='NN',_priority=INF,_fin_phrase=-np.inf):\n",
    "\t\tself.phrase = _phrase\n",
    "\t\tself.pos = _pos\n",
    "\t\tself.priority = _priority\n",
    "\t\tself.entity_id = -1\n",
    "\t\tself.fin_phrase = _fin_phrase\n",
    "\t\t\n",
    "\n",
    "\tdef __str__(self):\n",
    "\t\tres = \"[ phrase:%s\\n  pos: %s\\n  priority: %i\\n  entity_id: %i]\" % (self.phrase,self.pos,self.priority,self.entity_id)\n",
    "\t\treturn res\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\tres = \"[ phrase:%s\\n  pos: %s\\n  priority: %i\\n  entity_id: %i]\" % (self.phrase,self.pos,self.priority,self.entity_id)\n",
    "\t\treturn res\n",
    "\n",
    "'''\n",
    "read data and preprocess\n",
    "@param filename: relative or absolute training filename\n",
    "return\n",
    "\t(documents: [doc| doc=(entity,pos,entity_id)]\n",
    "\t vocabulary: list of entities as vocabulary\n",
    "\t)\n",
    "'''\n",
    "def getdata(filename,n_docs='all'):\n",
    "\n",
    "\tdocuments = []\n",
    "\tsentence = []\n",
    "\tvocabulary = set()\n",
    "\tcount_docs = 0\n",
    "\n",
    "\tfor line in open(filename):\n",
    "\t\tline = line.strip('\\n')\n",
    "\t\tif line.startswith('#begin'):\n",
    "\t\t\tdoc = []\n",
    "\t\t\tsentence=[]\n",
    "\t\t\tevent = Event()\n",
    "\t\t\topen_events = [event]\n",
    "\t\t\tcontinue\n",
    "\t\t\t#all temp files are reset\n",
    "\t\tif line.startswith('#end'):\n",
    "\t\t\t#save all temp docs\n",
    "\t\t\tdoc = np.array(doc)\n",
    "\t\t\tdocuments.append(doc)\n",
    "\t\t\tcount_docs+=1\n",
    "\t\t\tif n_docs!='all':\n",
    "\t\t\t\tif count_docs>=n_docs:\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\tcontinue\n",
    "\t\t\n",
    "\t\tif line!='':\n",
    "\t\t\ttemp1 = line.split('\\t')\n",
    "\t\t\tsentence.append(temp1)\n",
    "\n",
    "\t\tif line=='':\n",
    "\t\t\t# build tree\n",
    "\t\t\tnn = len(sentence)+1\n",
    "\t\t\tdep_graph = [[] for i in xrange(nn)]\n",
    "\t\t\tnodes_depth = np.zeros(nn)\n",
    "\t\t\troot = -1\n",
    "\t\t\tfor comp in sentence:\n",
    "\t\t\t\tu = int(comp[0])\n",
    "\t\t\t\tv = int(comp[8])\n",
    "\t\t\t\tdep_graph[u].append(v)\n",
    "\t\t\t\tdep_graph[v].append(u)\n",
    "\t\t\t\tif v==0:\n",
    "\t\t\t\t\troot=v\n",
    "\t\t\t# assign height to nodes with BFS\n",
    "\t\t\tqueue = [root]\n",
    "\t\t\tvisited = np.zeros(nn)\n",
    "\t\t\twhile len(queue)!=0:\n",
    "\t\t\t\tcurr_node = queue.pop()\n",
    "\t\t\t\tif visited[curr_node]!=1:\n",
    "\t\t\t\t\tvisited[curr_node]=1\n",
    "\t\t\t\t\tfor v in dep_graph[curr_node]:\n",
    "\t\t\t\t\t\tif visited[v]!=1 and nodes_depth[v]<nodes_depth[curr_node]+1:\n",
    "\t\t\t\t\t\t\tnodes_depth[v]=nodes_depth[curr_node]+1\n",
    "\t\t\t\t\t\t\tqueue.append(v)\n",
    "\n",
    "\t\t\t# Extract entities\n",
    "\t\t\tfor comp in sentence:\n",
    "\t\t\t\tevent = open_events[-1]\n",
    "\t\t\t\tn_tok = int(comp[0])\n",
    "\t\t\t\ttoken = comp[1]\n",
    "\t\t\t\tpos = comp[4]\n",
    "\t\t\t\tsynt_head = nodes_depth[n_tok] # corrected height\n",
    "\t\t\t\tcoref_str = comp[-1]\n",
    "\n",
    "\t\t\t\tif pos not in ['NNP','NNPS'] and token!=\"I\": # lowercase any other than proper nouns\n",
    "\t\t\t\t\ttoken = token.lower()\n",
    "\n",
    "\t\t\t\tfor i in xrange(len(open_events)):\n",
    "\t\t\t\t\tevent = open_events[i]\n",
    "\t\t\t\t\t# continue building phrase\n",
    "\t\t\t\t\tif pos[0]=='N' and pos==event.pos and n_tok==event.fin_phrase+1:\n",
    "\t\t\t\t\t\tif event.phrase!='':\n",
    "\t\t\t\t\t\t\tevent.phrase+=' '\n",
    "\t\t\t\t\t\tevent.phrase+=token\n",
    "\t\t\t\t\t\tevent.priority = min(event.priority,synt_head)\n",
    "\t\t\t\t\t\tevent.fin_phrase+=1\n",
    "\t\t\t\t\t\topen_events[i] = event\n",
    "\t\t\t\t\t# update ENTITY\n",
    "\t\t\t\t\telif pos in ['NN','NNS','NNP','NNPS','PRP','PRP$'] and synt_head<event.priority:\n",
    "\t\t\t\t\t\t\t# conserve ENTITY_ID\n",
    "\t\t\t\t\t\t\tnew_event = Event(token,pos,synt_head,n_tok)\n",
    "\t\t\t\t\t\t\tnew_event.entity_id = event.entity_id\n",
    "\t\t\t\t\t\t\topen_events[i] = new_event\n",
    "\n",
    "\t\t\t\tif coref_str=='_':\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\t# pos in ['NN','NNS','NNP','NNPS','PRP','PRP$'] and \n",
    "\t\t\t\ttemp = split_re.split(coref_str)\n",
    "\t\t\t\tsplitted = [a for a in temp if a!='']\n",
    "\t\t\t\tk=0\n",
    "\t\t\t\twhile(k<len(splitted)):\t\t# | no hace nada\n",
    "\t\t\t\t\tif splitted[k]=='(':\n",
    "\t\t\t\t\t\tpriority = np.inf\n",
    "\t\t\t\t\t\tif pos in ['NN','NNS','NNP','NNPS','PRP','PRP$']:\n",
    "\t\t\t\t\t\t\tpriority = synt_head\n",
    "\t\t\t\t\t\tevent = Event(token,pos,priority,n_tok)\t# initialization\n",
    "\t\t\t\t\t\topen_events.append(event)\n",
    "\t\t\t\t\telif splitted[k].isdigit():\n",
    "\t\t\t\t\t\tid = 0\n",
    "\t\t\t\t\t\tid = int(splitted[k])\n",
    "\t\t\t\t\t\topen_events[-1].entity_id = id # si es nuevo asigna, sino chanca el [mismo] id al ultimo activo\n",
    "\t\t\t\t\telif splitted[k]==')':\n",
    "\t\t\t\t\t\tif open_events[-1].pos in ['NN','NNS','NNP','NNPS','PRP','PRP$']:\n",
    "\t\t\t\t\t\t\ttext = open_events[-1].phrase\n",
    "\t\t\t\t\t\t\tev_pos = open_events[-1].pos\n",
    "\t\t\t\t\t\t\tent_id = open_events[-1].entity_id\n",
    "\t\t\t\t\t\t\tdoc.append(tuple([text,ev_pos,ent_id]) )\n",
    "\t\t\t\t\t\t\tvocabulary.add(open_events[-1].phrase)\t# build vocab\n",
    "\t\t\t\t\t\topen_events.pop()\n",
    "\t\t\t\t\tk+=1\n",
    "\t\t\t#END-FOR-ENTITIES\n",
    "\n",
    "\t\t\t#reset sentence var\n",
    "\t\t\tsentence = []\n",
    "\t\t#END-IF-EMPTY-LINE\n",
    "\t#END-FOR-FILE\n",
    "\tdocuments = np.array(documents)\n",
    "\tvocabulary = list(vocabulary)\n",
    "\treturn documents,vocabulary\n",
    "\n",
    "'''\n",
    "@param refs_by_doc: [doc | doc:{\"lex\":{ref:freq}, \"pro\":{ref:freq} }]\n",
    "return ( docs: {id | doc[id]:[ref_id]}\n",
    "\t\t refs_vocab: vocabulary of only referents)\n",
    "'''\n",
    "def index_docs(documents):\n",
    "\tN = len(documents)\n",
    "\tnew_documents = {}\n",
    "\trefs_vocab = []\n",
    "\tfor i in xrange(N):\n",
    "\t\tdoc = documents[i]\n",
    "\t\tnew_doc = []\n",
    "\t\tfor ref,freq in doc[\"lex\"].items():\n",
    "\t\t\tref_id = -1\n",
    "\t\t\tif ref not in refs_vocab:\n",
    "\t\t\t\tref_id = len(refs_vocab)\n",
    "\t\t\t\trefs_vocab.append(ref)\n",
    "\t\t\telse:\n",
    "\t\t\t\tref_id = refs_vocab.index(ref)\n",
    "\t\t\tnew_doc += freq*[ref_id]\n",
    "\n",
    "\t\tfor ref,freq in doc[\"pro\"].items():\n",
    "\t\t\tref_id = -1\n",
    "\t\t\tif ref not in refs_vocab:\n",
    "\t\t\t\tref_id = len(refs_vocab)\n",
    "\t\t\t\trefs_vocab.append(ref)\n",
    "\t\t\telse:\n",
    "\t\t\t\tref_id = refs_vocab.index(ref)\n",
    "\t\t\tnew_doc += freq*[ref_id]\n",
    "\t\tnew_documents[i]=new_doc\n",
    "\treturn new_documents,refs_vocab\n",
    "\n",
    "def add(_dict,key,val):\n",
    "\tif key not in _dict:\n",
    "\t\t_dict[key]=0\n",
    "\t_dict[key]+=val\n",
    "\treturn\n",
    "\n",
    "\"\"\"\n",
    "Extracts referent chains, assigns the referent and separate by lex or pro, + frequency\n",
    "return\n",
    "\trefs_by_doc: [doc | doc:{\"lex\":{ref:freq}, \"pro\":{ref:freq} }]\n",
    "\tref_by_token: {(pos_weight,token):[referents]}\n",
    "\"\"\"\n",
    "def get_referents(documents):\n",
    "\tN = documents.shape[0]\n",
    "\n",
    "\tref_by_token = {}\n",
    "\tpos_weights = {\n",
    "\t\t'NNP':0,\n",
    "\t\t'NNPS':0,\n",
    "\t\t'NN':5,\n",
    "\t\t'NNS':5,\n",
    "\t\t'PRP':10,\n",
    "\t\t'PRP$':10\n",
    "\t}\n",
    "\trefs_by_doc = []\n",
    "\tfor i in xrange(N):\n",
    "\t\tchains = {}\n",
    "\t\tlexical_noun_refs = {}\n",
    "\t\tpronoun_refs = {}\n",
    "\n",
    "\t\t# get annotated correference chains\n",
    "\t\tfor (ent,pos,ent_id) in documents[i]:\n",
    "\t\t\tif ent_id not in chains:\n",
    "\t\t\t\tchains[ent_id] = set()\n",
    "\t\t\tif (ent,pos) not in chains[ent_id]:\n",
    "\t\t\t\tchains[ent_id].add((pos_weights[pos],ent))\n",
    "\t\t\n",
    "\t\t# get lexical nouns and pronouns referents\n",
    "\t\tfor (ent_id,ref_chain) in chains.items():\n",
    "\t\t\tchain = set(ref_chain)\n",
    "\t\t\t(pos_w,ref) = chain.pop()\n",
    "\t\t\tif pos_w==10:\t# Only Nouns as referents\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# add to lexical_noun referents\n",
    "\t\t\tadd(lexical_noun_refs,ref,1)\n",
    "\t\t\t# map token to referent\n",
    "\t\t\ttoken = ref\n",
    "\t\t\tif (pos_w,token) not in ref_by_token:\n",
    "\t\t\t\tref_by_token[(pos_w,token)] = set()\n",
    "\t\t\tref_by_token[(pos_w,token)].add(ref)\n",
    "\n",
    "\t\t\twhile(len(chain)>0):\n",
    "\t\t\t\tpos_w,token = chain.pop()\n",
    "\t\t\t\tif pos_w==10:\n",
    "\t\t\t\t\tadd(pronoun_refs,ref,1)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tadd(lexical_noun_refs,ref,1)\n",
    "\t\t\t\t# map token to referent\n",
    "\t\t\t\tif (pos_w,token) not in ref_by_token:\n",
    "\t\t\t\t\tref_by_token[(pos_w,token)] = set()\n",
    "\t\t\t\tref_by_token[(pos_w,token)].add(ref)\n",
    "\n",
    "\t\t# save by doc\n",
    "\t\tdoc = {}\n",
    "\t\tdoc[\"lex\"] = lexical_noun_refs\n",
    "\t\tdoc[\"pro\"] = pronoun_refs\n",
    "\t\trefs_by_doc.append(doc)\n",
    "\treturn refs_by_doc,ref_by_token\n",
    "\t\t\n",
    "\"\"\"\n",
    "Assigns k from argmax p(xij|k,t). p(z_{ji}|G_j) = gamma * Gamma(gamma) * Gamma(m_{.k}) * fact(gamma-1) / fact(m_{..}+gamma-1)\n",
    "\"\"\"\n",
    "def log_pz(hdp,document_index,word,vocab):\n",
    "\tword_index = hdp._vocab.index(vocab.index(word))\n",
    "\tmax_topic_index = hdp.assign_topic_word(word_index)\n",
    "\n",
    "\tif max_topic_index==-1:\n",
    "\t\treturn -np.inf\n",
    "\t\t\n",
    "\t# p(t_{ij}=t)\n",
    "\t\"\"\"\n",
    "\ttable_log_likelihood = len(hdp._k_dt[document_index]) * np.log(hdp._alpha) - log_factorial(len(hdp._t_dv[document_index]), hdp._alpha);\n",
    "\tfor table_index in xrange(len(hdp._k_dt[document_index])):\n",
    "\t    table_log_likelihood += scipy.special.gammaln(hdp._n_dt[document_index][table_index]);\n",
    "\ttable_log_likelihood += scipy.special.gammaln(hdp._alpha);\n",
    "\t\"\"\"\n",
    "\n",
    "\t# p(k_{jt}=z_{ji}|t_{ji})\n",
    "\ttopic_log_likelihood = np.log(hdp._gamma) - log_factorial(np.sum(hdp._m_k), hdp._gamma)\n",
    "\ttopic_log_likelihood += scipy.special.gammaln(hdp._m_k[max_topic_index]);\n",
    "\ttopic_log_likelihood += scipy.special.gammaln(hdp._gamma);\n",
    "\n",
    "\treturn  topic_log_likelihood #* table_log_likelihood\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "p(z_{ji}|G_j) = n_{j.k} / Sum(n_{j..})\n",
    "\"\"\"\n",
    "def log_pz_ml(hdp,document_index,word,vocab):\n",
    "\tword_index = hdp._vocab.index(vocab.index(word))\n",
    "\tlog_pz = np.log(hdp._n_kd[:,document_index]) - np.log(np.sum(hdp._n_kd[:,document_index]))\n",
    "\tk = log_pz.argmax()\n",
    "\treturn log_pz.max()\n",
    "\n",
    "def expectation_ref(refs,hdp,doc_idx,vocab):\n",
    "\tE_log_pz = 0\n",
    "\tn_refs = 0\n",
    "\tfor ref,freq in refs.items():\n",
    "\t\tn_refs += freq\n",
    "\t\tE_log_pz += freq*log_pz(hdp,doc_idx,ref,vocab)\n",
    "\tif n_refs==0:\n",
    "\t\treturn -np.inf\n",
    "\tE_log_pz /= n_refs\n",
    "\treturn E_log_pz\n",
    "\n",
    "def expectation_ref_counts(refs,doc):\n",
    "\tE_log_pz = 0\n",
    "\tn_total = sum(doc[\"lex\"].values())\n",
    "\tn_total += sum(doc[\"pro\"].values())\n",
    "\tn_refs = 0\n",
    "\t\n",
    "\tfor ref,freq in refs.items():\n",
    "\t\tn_refs += freq\n",
    "\t\tE_log_pz += np.log(freq) - np.log(n_total)\t\t# p(ri) = C_{j,ri} / C_{j.}\n",
    "\n",
    "\tif n_refs==0:\n",
    "\t\treturn -np.inf\n",
    "\tE_log_pz /= n_refs\n",
    "\treturn E_log_pz\n",
    "\n",
    "'''\n",
    "@param refs_by_doc: [doc | doc:{\"lex\":{ref:freq}, \"pro\":{ref:freq} }]\n",
    "return \n",
    "'''\n",
    "def get_lr_format(refs_by_doc,hdp,refs_vocab):\n",
    "\t# model 1: only p_ri\n",
    "\t# model 2: p_ri; p(z|t,k)\n",
    "\tX = []\n",
    "\tY = []\n",
    "\tN = len(refs_by_doc)\n",
    "\tfor i in xrange(N):\n",
    "\t\tdoc = refs_by_doc[i]\n",
    "\t\tc_d = sum(doc[\"lex\"].values())\n",
    "\t\tc_d += sum(doc[\"pro\"].values())\n",
    "\t\tfor ref,freq in doc[\"lex\"].items():\n",
    "\t\t\tlog_pri = np.log(freq) - np.log(c_d)\n",
    "\t\t\tlog_pz_ref = log_pz(hdp,i,ref,refs_vocab)\n",
    "\n",
    "\t\t\tX.append([log_pri,log_pz_ref])\n",
    "\t\t\tY.append(0)\n",
    "\n",
    "\t\tfor ref,freq in doc[\"pro\"].items():\n",
    "\t\t\tlog_pri = np.log(freq) - np.log(c_d)\n",
    "\t\t\tlog_pz_ref = log_pz(hdp,i,ref,refs_vocab)\n",
    "\n",
    "\t\t\tX.append([log_pri,log_pz_ref])\n",
    "\t\t\tY.append(1)\n",
    "\treturn (X,Y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_docs=20\n",
    "gibbs_iterations=100\n",
    "gamma = 1.0\n",
    "alpha = 1.0\n",
    "eta = 0.01\n",
    "data_dir = \"../datasets/semeval_2010_t1_eng/data\"\n",
    "data_file = os.path.join(data_dir,'en.train.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and formating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs,vocab = getdata(data_file,n_docs=n_docs)\n",
    "# get referents by doc\n",
    "refs_by_doc,ref_by_token = get_referents(docs)\n",
    "# format for HDP\n",
    "formated_docs,referents_vocab = index_docs(refs_by_doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Hierarchical Dirichlet Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling in progress 10%\n",
      "total number of topics 54, log-likelihood is -9835.063389\n",
      "sampling in progress 20%\n",
      "total number of topics 59, log-likelihood is -9832.449102\n",
      "sampling in progress 30%\n",
      "total number of topics 59, log-likelihood is -9901.566471\n",
      "sampling in progress 40%\n",
      "total number of topics 54, log-likelihood is -9829.416023\n",
      "successfully export the snapshot to output/ for iteration 50...\n",
      "sampling in progress 50%\n",
      "total number of topics 61, log-likelihood is -9858.390769\n",
      "sampling in progress 60%\n",
      "total number of topics 55, log-likelihood is -9867.448619\n",
      "sampling in progress 70%\n",
      "total number of topics 59, log-likelihood is -9909.134023\n",
      "sampling in progress 80%\n",
      "total number of topics 58, log-likelihood is -9898.030937\n",
      "sampling in progress 90%\n",
      "total number of topics 59, log-likelihood is -9901.758595\n",
      "successfully export the snapshot to output/ for iteration 100...\n"
     ]
    }
   ],
   "source": [
    "gs = UncollapsedGibbsSampling(50);\n",
    "gs._initialize(formated_docs,gamma=gamma,alpha=alpha,eta=eta)\n",
    "gs.sample(iteration=gibbs_iterations,directory='output/');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top n words by topics, ordered by p(k)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k  | logp    | words\n",
      "------------------------------------------------\n",
      "  3| -364.45 |\tpeople, day, World, police, number, \n",
      " 18| -371.19 |\tseat, Thomas W. Field Jr., McKesson Corp., CNN Headline News, % stake, \n",
      "  9| -371.19 |\tyears, U.S., pictures, contract, billing, \n",
      "  8| -371.19 |\tWednesday, Libyans, court, thanks, issue, \n",
      "  4| -371.19 |\tfire, Gaza Strip, BBC News, head, Television, \n",
      " 37| -372.29 |\tdepth, growth, industries, time, election day, \n",
      " 31| -372.29 |\tweek, security co-operation, successes, negotiations, numbers, \n",
      " 24| -372.29 |\tadvertising, exports, gunmen, snapshot, chords, \n",
      " 17| -372.29 |\t%, unit, brands, necessity, exporter, \n",
      "  7| -372.29 |\tend, Mexico, link, agents, Brands, \n",
      "  6| -372.29 |\trum, Japan, Great Britain, marketing, ads, \n",
      " 57| -372.98 |\tassets, thrifts, deposits, withdrawals, securities, \n",
      " 56| -372.98 |\tThrift Supervision, rates, factors, mortgages, thrift industry, \n",
      " 55| -372.98 |\tunit, sale, paper company, Ferruzzi Agricola Finanziaria, interest, \n",
      " 54| -372.98 |\tfrancs, part, venture, affiliate, petrochemical group, \n"
     ]
    }
   ],
   "source": [
    "gs.print_topics(referents_vocab,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating topicality measure q_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Media test for E_pro and E_lex: p=0.0186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/matplotlib/backends/backend_gtk3.py:215: Warning: Source ID 928 was not found when attempting to remove it\n",
      "  GLib.source_remove(self._idle_event_id)\n"
     ]
    }
   ],
   "source": [
    "N = len(docs)\n",
    "e_lex = np.zeros(N)\n",
    "e_pro = np.zeros(N)\n",
    "# Calc E_pro and E_lex\n",
    "for doc_index in xrange(N):\n",
    "    E_lex = expectation_ref(refs_by_doc[doc_index][\"lex\"],gs,doc_index,referents_vocab)\n",
    "    E_pro = expectation_ref(refs_by_doc[doc_index][\"pro\"],gs,doc_index,referents_vocab)\n",
    "    e_lex[doc_index] = E_lex\n",
    "    e_pro[doc_index] = E_pro\n",
    "\n",
    "# Calc qd: topicalty measure\n",
    "qd = e_pro - e_lex\n",
    "# don't consider cases were e_pro==-INF\n",
    "noinf_idx = qd!=-np.inf\n",
    "# median test\n",
    "stat,p,m,_ = scipy.stats.median_test(e_pro[noinf_idx],e_lex[noinf_idx])\n",
    "\n",
    "print(\"\\nMedia test for E_pro and E_lex: p=%.4f\" % p)\n",
    "\n",
    "#p(q) as histogram\n",
    "plt.figure()\n",
    "plt.hist(qd[noinf_idx],20,normed=True)\n",
    "plt.title(\"p(q)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make table  | Target | Referent | topic_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                        Target |                       Referent | Ordered topic index\n",
      "----------------------------------------------------------------------------\n",
      "                      McKesson |                 McKesson Corp. | 1\n",
      "                     Mr. Field |            Thomas W. Field Jr. | 1\n",
      "                          Gaza |                     Gaza Strip | 4\n",
      "                  Hisham Mekki |                           head | 4\n",
      "                       England |                  Great Britain | 10\n",
      "                       Britain |                  Great Britain | 10\n",
      "      Southern Comfort Liqueur |               Southern Comfort | 10\n",
      "                Feldemuehle AG |                  paper company | 13\n",
      "                      Ferruzzi |  Ferruzzi Agricola Finanziaria | 13\n",
      "               Beghin-Say S.A. |                           unit | 13\n",
      "----------------------------------------------------------------------------\n",
      "                          they |                         police | 0\n",
      "                            us |                          World | 0\n",
      "                            it |                      War Plaza | 4\n",
      "                          they |                       stations | 9\n",
      "                           its |                          Japan | 10\n",
      "                            it |                          Japan | 10\n",
      "                         their |                        thrifts | 11\n",
      "                           its |                           unit | 13\n",
      "                            it |                           unit | 13\n",
      "                         their |                        lawyers | 17\n",
      "----------------------------------------------------------------------------\n",
      "                          post |                           seat | 1\n",
      "                      chairman |                           seat | 1\n",
      "          internationalization |                          shift | 10\n",
      "                           law |                    bailout law | 11\n",
      "                      industry |                thrift industry | 12\n",
      "                   transaction |                           sale | 13\n",
      "                       results |                           year | 15\n",
      "                        months |                        quarter | 15\n",
      "                        amount |                              % | 15\n",
      "                       company |                      Microsoft | 16\n",
      "----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# make: -pos_w,k_ord_idx,tuple_id\n",
    "target_ref_k = {} # pos_w: [k_ord,tupleid]\n",
    "token_ref_id = [] # token, ref as word_ids\n",
    "\n",
    "for ( (pos_w,token),refs ) in ref_by_token.items():\n",
    "    for ref in refs:\n",
    "        if ref==token:\n",
    "            continue\n",
    "        tr_idx = tuple([vocab.index(token),vocab.index(ref)])\t\t# using global vocabulary\n",
    "        tuple_id = -1\n",
    "        if tr_idx in token_ref_id:\n",
    "            tuple_id = token_ref_id.index(tr_idx)\n",
    "        else:\n",
    "            tuple_id = len(token_ref_id)\n",
    "            token_ref_id.append(tr_idx)\n",
    "\n",
    "        ref_id = gs._vocab.index(referents_vocab.index(ref))\t\t# get referent id in HDP vocab\n",
    "        k = gs.assign_topic_word(ref_id)\n",
    "        k_ord_idx = np.nonzero(gs._ord_topics_pk[:,1]==k)[0][0]\n",
    "        if pos_w not in target_ref_k:\n",
    "            target_ref_k[pos_w] = []\n",
    "        target_ref_k[pos_w].append( (k_ord_idx,tuple_id) )\n",
    "\n",
    "for key in target_ref_k.keys():\n",
    "    target_ref_k[key].sort()\n",
    "\n",
    "## plot: target | referent | topic_id\n",
    "max_per_pos = 10\n",
    "print(\"\\n%30s | %30s | Ordered topic index\" % (\"Target\",\"Referent\"))\n",
    "print(\"----------------------------------------------------------------------------\")\n",
    "for pos,tups in target_ref_k.items():\n",
    "    for i in xrange(max_per_pos):\n",
    "        k_ord_idx ,tup_id = tups[i]\n",
    "        target = vocab[ token_ref_id[tup_id][0] ]\n",
    "        ref    = vocab[ token_ref_id[tup_id][1] ]\n",
    "        #k = gs._ord_topics_pk[k_ord_idx,1]\n",
    "\n",
    "        print(\"%30s | %30s | %i\" % (target,ref,k_ord_idx))\n",
    "    print(\"----------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance tests for Qd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating q_d for p(ri|doc_d) from corpus...\n",
      "\n",
      "Media test for E_pro and E_lex: p=0.4328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/matplotlib/backends/backend_gtk3.py:215: Warning: Source ID 1107 was not found when attempting to remove it\n",
      "  GLib.source_remove(self._idle_event_id)\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "# Media test using prob referents directly from corpus (p_{ri}=c_{ri}/c_d)\n",
    "print(\"\\nCalculating q_d for p(ri|doc_d) from corpus...\")\t\n",
    "e_lex_corpus = np.zeros(N)\n",
    "e_pro_corpus = np.zeros(N)\n",
    "# Calc E_pro and E_lex\n",
    "for doc_index in xrange(N):\n",
    "    E_lex = expectation_ref_counts(refs_by_doc[doc_index][\"lex\"],refs_by_doc[doc_index])\n",
    "    E_pro = expectation_ref_counts(refs_by_doc[doc_index][\"pro\"],refs_by_doc[doc_index])\n",
    "    e_lex_corpus[doc_index] = E_lex\n",
    "    e_pro_corpus[doc_index] = E_pro\n",
    "\n",
    "# Calc qd: topicalty measure\n",
    "qd_corpus = e_pro_corpus - e_lex_corpus\n",
    "# don't consider cases were e_pro==-INF\n",
    "noinf_idx = qd_corpus!=-np.inf\n",
    "# median test\n",
    "stat,p,m,_ = scipy.stats.median_test(e_pro_corpus[noinf_idx],e_lex_corpus[noinf_idx])\n",
    "print(\"\\nMedia test for E_pro and E_lex: p=%.4f\" % p)\n",
    "\n",
    "#p(qd_corpus) as histogram\n",
    "plt.figure()\n",
    "plt.hist(qd_corpus[noinf_idx],20,normed=True,histtype='stepfilled')\n",
    "plt.title(\"p(q')\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Logistic Regression models setup\n",
    "# get features for models 1 and 2 and export to csv for further analysis in R\n",
    "x,y = get_lr_format(refs_by_doc,gs,referents_vocab)\n",
    "output = open(\"lr_models.csv\",'w')\n",
    "output.write(\"Y,X1,X2\\n\")\n",
    "for i in xrange(len(y)):\n",
    "    output.write(\"%i,%.4f,%.4f\\n\" % (y[i],x[i][0],x[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
