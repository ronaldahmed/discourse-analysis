{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %load utils_topicality.py\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import ipdb\n",
    "\n",
    "\n",
    "data_dir = \"../datasets/semeval_2010_t1_eng/data\"\n",
    "#libs_dir = \"hdp\"\n",
    "#sys.path.append(libs_dir)\n",
    "from hdp.ugs import *\n",
    "\n",
    "numpy.seterr(divide='ignore')\n",
    "\n",
    "split_re = re.compile(r'([()|])')\n",
    "INF = 1e10\n",
    "\n",
    "class Event:\n",
    "\tdef __init__(self,_phrase='',_pos='NN',_priority=INF,_fin_phrase=-np.inf):\n",
    "\t\tself.phrase = _phrase\n",
    "\t\tself.pos = _pos\n",
    "\t\tself.priority = _priority\n",
    "\t\tself.entity_id = -1\n",
    "\t\tself.fin_phrase = _fin_phrase\n",
    "\t\t\n",
    "\n",
    "\tdef __str__(self):\n",
    "\t\tres = \"[ phrase:%s\\n  pos: %s\\n  priority: %i\\n  entity_id: %i]\" % (self.phrase,self.pos,self.priority,self.entity_id)\n",
    "\t\treturn res\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\tres = \"[ phrase:%s\\n  pos: %s\\n  priority: %i\\n  entity_id: %i]\" % (self.phrase,self.pos,self.priority,self.entity_id)\n",
    "\t\treturn res\n",
    "\n",
    "'''\n",
    "read data and preprocess\n",
    "@param filename: relative or absolute training filename\n",
    "return\n",
    "\t(documents: [doc| doc=(entity,pos,entity_id)]\n",
    "\t vocabulary: list of entities as vocabulary\n",
    "\t)\n",
    "'''\n",
    "def getdata(filename,n_docs='all'):\n",
    "\n",
    "\tdocuments = []\n",
    "\tsentence = []\n",
    "\tvocabulary = set()\n",
    "\tcount_docs = 0\n",
    "\n",
    "\tfor line in open(filename):\n",
    "\t\tline = line.strip('\\n')\n",
    "\t\tif line.startswith('#begin'):\n",
    "\t\t\tdoc = []\n",
    "\t\t\tsentence=[]\n",
    "\t\t\tevent = Event()\n",
    "\t\t\topen_events = [event]\n",
    "\t\t\tcontinue\n",
    "\t\t\t#all temp files are reset\n",
    "\t\tif line.startswith('#end'):\n",
    "\t\t\t#save all temp docs\n",
    "\t\t\tdoc = np.array(doc)\n",
    "\t\t\tdocuments.append(doc)\n",
    "\t\t\tcount_docs+=1\n",
    "\t\t\tif n_docs!='all':\n",
    "\t\t\t\tif count_docs>=n_docs:\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\tcontinue\n",
    "\t\t\n",
    "\t\tif line!='':\n",
    "\t\t\ttemp1 = line.split('\\t')\n",
    "\t\t\tsentence.append(temp1)\n",
    "\n",
    "\t\tif line=='':\n",
    "\t\t\t# build tree\n",
    "\t\t\tnn = len(sentence)+1\n",
    "\t\t\tdep_graph = [[] for i in xrange(nn)]\n",
    "\t\t\tnodes_depth = np.zeros(nn)\n",
    "\t\t\troot = -1\n",
    "\t\t\tfor comp in sentence:\n",
    "\t\t\t\tu = int(comp[0])\n",
    "\t\t\t\tv = int(comp[8])\n",
    "\t\t\t\tdep_graph[u].append(v)\n",
    "\t\t\t\tdep_graph[v].append(u)\n",
    "\t\t\t\tif v==0:\n",
    "\t\t\t\t\troot=v\n",
    "\t\t\t# assign height to nodes with BFS\n",
    "\t\t\tqueue = [root]\n",
    "\t\t\tvisited = np.zeros(nn)\n",
    "\t\t\twhile len(queue)!=0:\n",
    "\t\t\t\tcurr_node = queue.pop()\n",
    "\t\t\t\tif visited[curr_node]!=1:\n",
    "\t\t\t\t\tvisited[curr_node]=1\n",
    "\t\t\t\t\tfor v in dep_graph[curr_node]:\n",
    "\t\t\t\t\t\tif visited[v]!=1 and nodes_depth[v]<nodes_depth[curr_node]+1:\n",
    "\t\t\t\t\t\t\tnodes_depth[v]=nodes_depth[curr_node]+1\n",
    "\t\t\t\t\t\t\tqueue.append(v)\n",
    "\n",
    "\t\t\t# Extract entities\n",
    "\t\t\tfor comp in sentence:\n",
    "\t\t\t\tevent = open_events[-1]\n",
    "\t\t\t\tn_tok = int(comp[0])\n",
    "\t\t\t\ttoken = comp[1]\n",
    "\t\t\t\tpos = comp[4]\n",
    "\t\t\t\tsynt_head = nodes_depth[n_tok] # corrected height\n",
    "\t\t\t\tcoref_str = comp[-1]\n",
    "\n",
    "\t\t\t\tif pos not in ['NNP','NNPS'] and token!=\"I\": # lowercase any other than proper nouns\n",
    "\t\t\t\t\ttoken = token.lower()\n",
    "\n",
    "\t\t\t\tfor i in xrange(len(open_events)):\n",
    "\t\t\t\t\tevent = open_events[i]\n",
    "\t\t\t\t\t# continue building phrase\n",
    "\t\t\t\t\tif pos[0]=='N' and pos==event.pos and n_tok==event.fin_phrase+1:\n",
    "\t\t\t\t\t\tif event.phrase!='':\n",
    "\t\t\t\t\t\t\tevent.phrase+=' '\n",
    "\t\t\t\t\t\tevent.phrase+=token\n",
    "\t\t\t\t\t\tevent.priority = min(event.priority,synt_head)\n",
    "\t\t\t\t\t\tevent.fin_phrase+=1\n",
    "\t\t\t\t\t\topen_events[i] = event\n",
    "\t\t\t\t\t# update ENTITY\n",
    "\t\t\t\t\telif pos in ['NN','NNS','NNP','NNPS','PRP','PRP$'] and synt_head<event.priority:\n",
    "\t\t\t\t\t\t\t# conserve ENTITY_ID\n",
    "\t\t\t\t\t\t\tnew_event = Event(token,pos,synt_head,n_tok)\n",
    "\t\t\t\t\t\t\tnew_event.entity_id = event.entity_id\n",
    "\t\t\t\t\t\t\topen_events[i] = new_event\n",
    "\n",
    "\t\t\t\tif coref_str=='_':\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\t# pos in ['NN','NNS','NNP','NNPS','PRP','PRP$'] and \n",
    "\t\t\t\ttemp = split_re.split(coref_str)\n",
    "\t\t\t\tsplitted = [a for a in temp if a!='']\n",
    "\t\t\t\tk=0\n",
    "\t\t\t\twhile(k<len(splitted)):\t\t# | no hace nada\n",
    "\t\t\t\t\tif splitted[k]=='(':\n",
    "\t\t\t\t\t\tpriority = np.inf\n",
    "\t\t\t\t\t\tif pos in ['NN','NNS','NNP','NNPS','PRP','PRP$']:\n",
    "\t\t\t\t\t\t\tpriority = synt_head\n",
    "\t\t\t\t\t\tevent = Event(token,pos,priority,n_tok)\t# initialization\n",
    "\t\t\t\t\t\topen_events.append(event)\n",
    "\t\t\t\t\telif splitted[k].isdigit():\n",
    "\t\t\t\t\t\tid = 0\n",
    "\t\t\t\t\t\tid = int(splitted[k])\n",
    "\t\t\t\t\t\topen_events[-1].entity_id = id # si es nuevo asigna, sino chanca el [mismo] id al ultimo activo\n",
    "\t\t\t\t\telif splitted[k]==')':\n",
    "\t\t\t\t\t\tif open_events[-1].pos in ['NN','NNS','NNP','NNPS','PRP','PRP$']:\n",
    "\t\t\t\t\t\t\ttext = open_events[-1].phrase\n",
    "\t\t\t\t\t\t\tev_pos = open_events[-1].pos\n",
    "\t\t\t\t\t\t\tent_id = open_events[-1].entity_id\n",
    "\t\t\t\t\t\t\tdoc.append(tuple([text,ev_pos,ent_id]) )\n",
    "\t\t\t\t\t\t\tvocabulary.add(open_events[-1].phrase)\t# build vocab\n",
    "\t\t\t\t\t\topen_events.pop()\n",
    "\t\t\t\t\tk+=1\n",
    "\t\t\t#END-FOR-ENTITIES\n",
    "\n",
    "\t\t\t#reset sentence var\n",
    "\t\t\tsentence = []\n",
    "\t\t#END-IF-EMPTY-LINE\n",
    "\t#END-FOR-FILE\n",
    "\tdocuments = np.array(documents)\n",
    "\tvocabulary = list(vocabulary)\n",
    "\treturn documents,vocabulary\n",
    "\n",
    "'''\n",
    "@param refs_by_doc: [doc | doc:{\"lex\":{ref:freq}, \"pro\":{ref:freq} }]\n",
    "return ( docs: {id | doc[id]:[ref_id]}\n",
    "\t\t refs_vocab: vocabulary of only referents)\n",
    "'''\n",
    "def index_docs(documents):\n",
    "\tN = len(documents)\n",
    "\tnew_documents = {}\n",
    "\trefs_vocab = []\n",
    "\tfor i in xrange(N):\n",
    "\t\tdoc = documents[i]\n",
    "\t\tnew_doc = []\n",
    "\t\tfor ref,freq in doc[\"lex\"].items():\n",
    "\t\t\tref_id = -1\n",
    "\t\t\tif ref not in refs_vocab:\n",
    "\t\t\t\tref_id = len(refs_vocab)\n",
    "\t\t\t\trefs_vocab.append(ref)\n",
    "\t\t\telse:\n",
    "\t\t\t\tref_id = refs_vocab.index(ref)\n",
    "\t\t\tnew_doc += freq*[ref_id]\n",
    "\n",
    "\t\tfor ref,freq in doc[\"pro\"].items():\n",
    "\t\t\tref_id = -1\n",
    "\t\t\tif ref not in refs_vocab:\n",
    "\t\t\t\tref_id = len(refs_vocab)\n",
    "\t\t\t\trefs_vocab.append(ref)\n",
    "\t\t\telse:\n",
    "\t\t\t\tref_id = refs_vocab.index(ref)\n",
    "\t\t\tnew_doc += freq*[ref_id]\n",
    "\t\tnew_documents[i]=new_doc\n",
    "\treturn new_documents,refs_vocab\n",
    "\n",
    "def add(_dict,key,val):\n",
    "\tif key not in _dict:\n",
    "\t\t_dict[key]=0\n",
    "\t_dict[key]+=val\n",
    "\treturn\n",
    "\n",
    "\"\"\"\n",
    "Extracts referent chains, assigns the referent and separate by lex or pro, + frequency\n",
    "return\n",
    "\trefs_by_doc: [doc | doc:{\"lex\":{ref:freq}, \"pro\":{ref:freq} }]\n",
    "\tref_by_token: {(pos_weight,token):[referents]}\n",
    "\"\"\"\n",
    "def get_referents(documents):\n",
    "\tN = documents.shape[0]\n",
    "\n",
    "\tref_by_token = {}\n",
    "\tpos_weights = {\n",
    "\t\t'NNP':0,\n",
    "\t\t'NNPS':0,\n",
    "\t\t'NN':5,\n",
    "\t\t'NNS':5,\n",
    "\t\t'PRP':10,\n",
    "\t\t'PRP$':10\n",
    "\t}\n",
    "\trefs_by_doc = []\n",
    "\tfor i in xrange(N):\n",
    "\t\tchains = {}\n",
    "\t\tlexical_noun_refs = {}\n",
    "\t\tpronoun_refs = {}\n",
    "\n",
    "\t\t# get annotated correference chains\n",
    "\t\tfor (ent,pos,ent_id) in documents[i]:\n",
    "\t\t\tif ent_id not in chains:\n",
    "\t\t\t\tchains[ent_id] = set()\n",
    "\t\t\tif (ent,pos) not in chains[ent_id]:\n",
    "\t\t\t\tchains[ent_id].add((pos_weights[pos],ent))\n",
    "\t\t\n",
    "\t\t# get lexical nouns and pronouns referents\n",
    "\t\tfor (ent_id,ref_chain) in chains.items():\n",
    "\t\t\tchain = set(ref_chain)\n",
    "\t\t\t(pos_w,ref) = chain.pop()\n",
    "\t\t\tif pos_w==10:\t# Only Nouns as referents\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# add to lexical_noun referents\n",
    "\t\t\tadd(lexical_noun_refs,ref,1)\n",
    "\t\t\t# map token to referent\n",
    "\t\t\ttoken = ref\n",
    "\t\t\tif (pos_w,token) not in ref_by_token:\n",
    "\t\t\t\tref_by_token[(pos_w,token)] = set()\n",
    "\t\t\tref_by_token[(pos_w,token)].add(ref)\n",
    "\n",
    "\t\t\twhile(len(chain)>0):\n",
    "\t\t\t\tpos_w,token = chain.pop()\n",
    "\t\t\t\tif pos_w==10:\n",
    "\t\t\t\t\tadd(pronoun_refs,ref,1)\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tadd(lexical_noun_refs,ref,1)\n",
    "\t\t\t\t# map token to referent\n",
    "\t\t\t\tif (pos_w,token) not in ref_by_token:\n",
    "\t\t\t\t\tref_by_token[(pos_w,token)] = set()\n",
    "\t\t\t\tref_by_token[(pos_w,token)].add(ref)\n",
    "\n",
    "\t\t# save by doc\n",
    "\t\tdoc = {}\n",
    "\t\tdoc[\"lex\"] = lexical_noun_refs\n",
    "\t\tdoc[\"pro\"] = pronoun_refs\n",
    "\t\trefs_by_doc.append(doc)\n",
    "\treturn refs_by_doc,ref_by_token\n",
    "\t\t\n",
    "\"\"\"\n",
    "Assigns k from argmax p(xij|k,t). p(z_{ji}|G_j) = gamma * Gamma(gamma) * Gamma(m_{.k}) * fact(gamma-1) / fact(m_{..}+gamma-1)\n",
    "\"\"\"\n",
    "def log_pz(hdp,document_index,word,vocab):\n",
    "\tword_index = hdp._vocab.index(vocab.index(word))\n",
    "\tmax_topic_index = hdp.assign_topic_word(word_index)\n",
    "\n",
    "\tif max_topic_index==-1:\n",
    "\t\treturn -np.inf\n",
    "\t\t\n",
    "\t# p(t_{ij}=t)\n",
    "\t\"\"\"\n",
    "\ttable_log_likelihood = len(hdp._k_dt[document_index]) * np.log(hdp._alpha) - log_factorial(len(hdp._t_dv[document_index]), hdp._alpha);\n",
    "\tfor table_index in xrange(len(hdp._k_dt[document_index])):\n",
    "\t    table_log_likelihood += scipy.special.gammaln(hdp._n_dt[document_index][table_index]);\n",
    "\ttable_log_likelihood += scipy.special.gammaln(hdp._alpha);\n",
    "\t\"\"\"\n",
    "\n",
    "\t# p(k_{jt}=z_{ji}|t_{ji})\n",
    "\ttopic_log_likelihood = np.log(hdp._gamma) - log_factorial(np.sum(hdp._m_k), hdp._gamma)\n",
    "\ttopic_log_likelihood += scipy.special.gammaln(hdp._m_k[max_topic_index]);\n",
    "\ttopic_log_likelihood += scipy.special.gammaln(hdp._gamma);\n",
    "\n",
    "\treturn  topic_log_likelihood #* table_log_likelihood\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "p(z_{ji}|G_j) = n_{j.k} / Sum(n_{j..})\n",
    "\"\"\"\n",
    "def log_pz_ml(hdp,document_index,word,vocab):\n",
    "\tword_index = hdp._vocab.index(vocab.index(word))\n",
    "\tlog_pz = np.log(hdp._n_kd[:,document_index]) - np.log(np.sum(hdp._n_kd[:,document_index]))\n",
    "\tk = log_pz.argmax()\n",
    "\treturn log_pz.max()\n",
    "\n",
    "def expectation_ref(refs,hdp,doc_idx,vocab):\n",
    "\tE_log_pz = 0\n",
    "\tn_refs = 0\n",
    "\tfor ref,freq in refs.items():\n",
    "\t\tn_refs += freq\n",
    "\t\tE_log_pz += freq*log_pz(hdp,doc_idx,ref,vocab)\n",
    "\tif n_refs==0:\n",
    "\t\treturn -np.inf\n",
    "\tE_log_pz /= n_refs\n",
    "\treturn E_log_pz\n",
    "\n",
    "def expectation_ref_counts(refs,doc):\n",
    "\tE_log_pz = 0\n",
    "\tn_total = sum(doc[\"lex\"].values())\n",
    "\tn_total += sum(doc[\"pro\"].values())\n",
    "\tn_refs = 0\n",
    "\t\n",
    "\tfor ref,freq in refs.items():\n",
    "\t\tn_refs += freq\n",
    "\t\tE_log_pz += np.log(freq) - np.log(n_total)\t\t# p(ri) = C_{j,ri} / C_{j.}\n",
    "\n",
    "\tif n_refs==0:\n",
    "\t\treturn -np.inf\n",
    "\tE_log_pz /= n_refs\n",
    "\treturn E_log_pz\n",
    "\n",
    "'''\n",
    "@param refs_by_doc: [doc | doc:{\"lex\":{ref:freq}, \"pro\":{ref:freq} }]\n",
    "return \n",
    "'''\n",
    "def get_lr_format(refs_by_doc,hdp,refs_vocab):\n",
    "\t# model 1: only p_ri\n",
    "\t# model 2: p_ri; p(z|t,k)\n",
    "\tX = []\n",
    "\tY = []\n",
    "\tN = len(refs_by_doc)\n",
    "\tfor i in xrange(N):\n",
    "\t\tdoc = refs_by_doc[i]\n",
    "\t\tc_d = sum(doc[\"lex\"].values())\n",
    "\t\tc_d += sum(doc[\"pro\"].values())\n",
    "\t\tfor ref,freq in doc[\"lex\"].items():\n",
    "\t\t\tlog_pri = np.log(freq) - np.log(c_d)\n",
    "\t\t\tlog_pz_ref = log_pz(hdp,i,ref,refs_vocab)\n",
    "\n",
    "\t\t\tX.append([log_pri,log_pz_ref])\n",
    "\t\t\tY.append(0)\n",
    "\n",
    "\t\tfor ref,freq in doc[\"pro\"].items():\n",
    "\t\t\tlog_pri = np.log(freq) - np.log(c_d)\n",
    "\t\t\tlog_pz_ref = log_pz(hdp,i,ref,refs_vocab)\n",
    "\n",
    "\t\t\tX.append([log_pri,log_pz_ref])\n",
    "\t\t\tY.append(1)\n",
    "\treturn (X,Y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_docs=\"all\"\n",
    "gibbs_iterations=1000\n",
    "gamma = 1.0\n",
    "alpha = 1.0\n",
    "eta = 0.01\n",
    "data_dir = \"../datasets/semeval_2010_t1_eng/data\"\n",
    "data_file = os.path.join(data_dir,'en.train.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and formating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs,vocab = getdata(data_file,n_docs=n_docs)\n",
    "# get referents by doc\n",
    "refs_by_doc,ref_by_token = get_referents(docs)\n",
    "# format for HDP\n",
    "formated_docs,referents_vocab = index_docs(refs_by_doc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Hierarchical Dirichlet Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampling in progress  1%\n",
      "total number of topics 154, log-likelihood is -134667.596852\n",
      "sampling in progress  2%\n",
      "total number of topics 158, log-likelihood is -134279.396978\n",
      "sampling in progress  3%\n",
      "total number of topics 164, log-likelihood is -133728.014652\n",
      "sampling in progress  4%\n",
      "total number of topics 167, log-likelihood is -133845.697866\n",
      "successfully export the snapshot to output/ for iteration 50...\n",
      "sampling in progress  5%\n",
      "total number of topics 165, log-likelihood is -133813.828435\n",
      "sampling in progress  6%\n",
      "total number of topics 165, log-likelihood is -133942.390110\n",
      "sampling in progress  7%\n",
      "total number of topics 165, log-likelihood is -133665.073731\n",
      "sampling in progress  8%\n",
      "total number of topics 161, log-likelihood is -133369.146823\n",
      "sampling in progress  9%\n",
      "total number of topics 164, log-likelihood is -133365.473300\n",
      "successfully export the snapshot to output/ for iteration 100...\n",
      "sampling in progress 10%\n",
      "total number of topics 165, log-likelihood is -133317.078463\n",
      "sampling in progress 11%\n",
      "total number of topics 168, log-likelihood is -133176.020016\n",
      "sampling in progress 12%\n",
      "total number of topics 163, log-likelihood is -133071.848219\n",
      "sampling in progress 13%\n",
      "total number of topics 170, log-likelihood is -133304.073603\n",
      "sampling in progress 14%\n",
      "total number of topics 165, log-likelihood is -133019.083101\n",
      "successfully export the snapshot to output/ for iteration 150...\n",
      "sampling in progress 15%\n",
      "total number of topics 168, log-likelihood is -133171.602153\n",
      "sampling in progress 16%\n",
      "total number of topics 164, log-likelihood is -133048.642930\n",
      "sampling in progress 17%\n",
      "total number of topics 167, log-likelihood is -133078.458782\n",
      "sampling in progress 18%\n",
      "total number of topics 166, log-likelihood is -132629.320977\n",
      "sampling in progress 19%\n",
      "total number of topics 171, log-likelihood is -133077.624153\n",
      "successfully export the snapshot to output/ for iteration 200...\n",
      "sampling in progress 20%\n",
      "total number of topics 169, log-likelihood is -133303.790101\n",
      "sampling in progress 21%\n",
      "total number of topics 166, log-likelihood is -133411.519692\n",
      "sampling in progress 22%\n",
      "total number of topics 169, log-likelihood is -133284.076625\n",
      "sampling in progress 23%\n",
      "total number of topics 171, log-likelihood is -132598.887143\n",
      "sampling in progress 24%\n",
      "total number of topics 168, log-likelihood is -133188.518236\n",
      "successfully export the snapshot to output/ for iteration 250...\n",
      "sampling in progress 25%\n",
      "total number of topics 164, log-likelihood is -132773.445520\n",
      "sampling in progress 26%\n",
      "total number of topics 161, log-likelihood is -132873.786766\n",
      "sampling in progress 27%\n",
      "total number of topics 168, log-likelihood is -132815.367195\n",
      "sampling in progress 28%\n",
      "total number of topics 168, log-likelihood is -132502.722392\n",
      "sampling in progress 29%\n",
      "total number of topics 172, log-likelihood is -133392.925409\n",
      "successfully export the snapshot to output/ for iteration 300...\n",
      "sampling in progress 30%\n",
      "total number of topics 169, log-likelihood is -133312.804832\n",
      "sampling in progress 31%\n",
      "total number of topics 165, log-likelihood is -132390.333035\n",
      "sampling in progress 32%\n",
      "total number of topics 165, log-likelihood is -132918.341214\n",
      "sampling in progress 33%\n",
      "total number of topics 163, log-likelihood is -132746.579004\n",
      "sampling in progress 34%\n",
      "total number of topics 162, log-likelihood is -133309.990493\n",
      "successfully export the snapshot to output/ for iteration 350...\n",
      "sampling in progress 35%\n",
      "total number of topics 166, log-likelihood is -132970.949170\n",
      "sampling in progress 36%\n",
      "total number of topics 164, log-likelihood is -132865.648117\n",
      "sampling in progress 37%\n",
      "total number of topics 166, log-likelihood is -133045.836777\n",
      "sampling in progress 38%\n",
      "total number of topics 162, log-likelihood is -133012.727447\n",
      "sampling in progress 39%\n",
      "total number of topics 163, log-likelihood is -132957.497146\n",
      "successfully export the snapshot to output/ for iteration 400...\n",
      "sampling in progress 40%\n",
      "total number of topics 166, log-likelihood is -133005.891610\n",
      "sampling in progress 41%\n",
      "total number of topics 166, log-likelihood is -132742.186351\n",
      "sampling in progress 42%\n",
      "total number of topics 166, log-likelihood is -132887.776826\n",
      "sampling in progress 43%\n",
      "total number of topics 171, log-likelihood is -133195.893582\n",
      "sampling in progress 44%\n",
      "total number of topics 166, log-likelihood is -133059.312012\n",
      "successfully export the snapshot to output/ for iteration 450...\n",
      "sampling in progress 45%\n",
      "total number of topics 167, log-likelihood is -133145.312074\n",
      "sampling in progress 46%\n",
      "total number of topics 168, log-likelihood is -132887.331126\n",
      "sampling in progress 47%\n",
      "total number of topics 167, log-likelihood is -132975.959373\n",
      "sampling in progress 48%\n",
      "total number of topics 161, log-likelihood is -132735.837850\n",
      "sampling in progress 49%\n",
      "total number of topics 168, log-likelihood is -132619.273569\n",
      "successfully export the snapshot to output/ for iteration 500...\n",
      "sampling in progress 50%\n",
      "total number of topics 165, log-likelihood is -132846.664237\n",
      "sampling in progress 51%\n",
      "total number of topics 161, log-likelihood is -132730.935332\n",
      "sampling in progress 52%\n",
      "total number of topics 164, log-likelihood is -132634.431674\n",
      "sampling in progress 53%\n",
      "total number of topics 163, log-likelihood is -132593.388432\n",
      "sampling in progress 54%\n",
      "total number of topics 164, log-likelihood is -132808.128056\n",
      "successfully export the snapshot to output/ for iteration 550...\n",
      "sampling in progress 55%\n",
      "total number of topics 159, log-likelihood is -132847.747924\n",
      "sampling in progress 56%\n",
      "total number of topics 161, log-likelihood is -132991.643264\n",
      "sampling in progress 57%\n",
      "total number of topics 157, log-likelihood is -132890.352566\n",
      "sampling in progress 58%\n",
      "total number of topics 166, log-likelihood is -132901.040546\n",
      "sampling in progress 59%\n",
      "total number of topics 162, log-likelihood is -133005.994621\n",
      "successfully export the snapshot to output/ for iteration 600...\n",
      "sampling in progress 60%\n",
      "total number of topics 160, log-likelihood is -132884.207020\n",
      "sampling in progress 61%\n",
      "total number of topics 159, log-likelihood is -132535.356326\n",
      "sampling in progress 62%\n",
      "total number of topics 161, log-likelihood is -132398.286225\n",
      "sampling in progress 63%\n",
      "total number of topics 161, log-likelihood is -132970.920438\n",
      "sampling in progress 64%\n",
      "total number of topics 164, log-likelihood is -132835.555260\n",
      "successfully export the snapshot to output/ for iteration 650...\n",
      "sampling in progress 65%\n",
      "total number of topics 167, log-likelihood is -132770.075251\n",
      "sampling in progress 66%\n",
      "total number of topics 165, log-likelihood is -132569.202788\n",
      "sampling in progress 67%\n",
      "total number of topics 161, log-likelihood is -132481.497634\n",
      "sampling in progress 68%\n",
      "total number of topics 160, log-likelihood is -132616.114151\n",
      "sampling in progress 69%\n",
      "total number of topics 161, log-likelihood is -133001.473223\n",
      "successfully export the snapshot to output/ for iteration 700...\n",
      "sampling in progress 70%\n",
      "total number of topics 159, log-likelihood is -132648.926962\n",
      "sampling in progress 71%\n",
      "total number of topics 163, log-likelihood is -132718.304594\n",
      "sampling in progress 72%\n",
      "total number of topics 161, log-likelihood is -132609.713525\n",
      "sampling in progress 73%\n",
      "total number of topics 161, log-likelihood is -132628.073345\n",
      "sampling in progress 74%\n",
      "total number of topics 159, log-likelihood is -132384.678031\n",
      "successfully export the snapshot to output/ for iteration 750...\n",
      "sampling in progress 75%\n",
      "total number of topics 159, log-likelihood is -132378.834485\n",
      "sampling in progress 76%\n",
      "total number of topics 161, log-likelihood is -132867.804490\n",
      "sampling in progress 77%\n",
      "total number of topics 165, log-likelihood is -132496.795913\n",
      "sampling in progress 78%\n",
      "total number of topics 165, log-likelihood is -132790.922309\n",
      "sampling in progress 79%\n",
      "total number of topics 166, log-likelihood is -132809.907307\n",
      "successfully export the snapshot to output/ for iteration 800...\n",
      "sampling in progress 80%\n",
      "total number of topics 168, log-likelihood is -133112.747372\n",
      "sampling in progress 81%\n",
      "total number of topics 169, log-likelihood is -133184.265376\n",
      "sampling in progress 82%\n",
      "total number of topics 167, log-likelihood is -133214.187057\n",
      "sampling in progress 83%\n",
      "total number of topics 164, log-likelihood is -133116.245611\n",
      "sampling in progress 84%\n",
      "total number of topics 164, log-likelihood is -132638.805863\n",
      "successfully export the snapshot to output/ for iteration 850...\n",
      "sampling in progress 85%\n",
      "total number of topics 162, log-likelihood is -132763.370036\n",
      "sampling in progress 86%\n",
      "total number of topics 167, log-likelihood is -132898.030478\n",
      "sampling in progress 87%\n",
      "total number of topics 166, log-likelihood is -132919.856978\n",
      "sampling in progress 88%\n",
      "total number of topics 166, log-likelihood is -133166.030890\n",
      "sampling in progress 89%\n",
      "total number of topics 166, log-likelihood is -132413.513635\n",
      "successfully export the snapshot to output/ for iteration 900...\n",
      "sampling in progress 90%\n",
      "total number of topics 163, log-likelihood is -132598.448640\n",
      "sampling in progress 91%\n",
      "total number of topics 170, log-likelihood is -132856.757109\n",
      "sampling in progress 92%\n",
      "total number of topics 166, log-likelihood is -132758.734367\n",
      "sampling in progress 93%\n",
      "total number of topics 162, log-likelihood is -132929.145282\n",
      "sampling in progress 94%\n",
      "total number of topics 162, log-likelihood is -132530.450890\n",
      "successfully export the snapshot to output/ for iteration 950...\n",
      "sampling in progress 95%\n",
      "total number of topics 160, log-likelihood is -132629.676304\n",
      "sampling in progress 96%\n",
      "total number of topics 162, log-likelihood is -132880.666777\n",
      "sampling in progress 97%\n",
      "total number of topics 163, log-likelihood is -132791.041396\n",
      "sampling in progress 98%\n",
      "total number of topics 164, log-likelihood is -132669.984522\n",
      "sampling in progress 99%\n",
      "total number of topics 165, log-likelihood is -132732.053646\n",
      "successfully export the snapshot to output/ for iteration 1000...\n"
     ]
    }
   ],
   "source": [
    "gs = UncollapsedGibbsSampling(50);\n",
    "gs._initialize(formated_docs,gamma=gamma,alpha=alpha,eta=eta)\n",
    "gs.sample(iteration=gibbs_iterations,directory='output/');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top n words by topics, ordered by p(k)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k  | logp    | words\n",
      "------------------------------------------------\n",
      "  0| -5267.30 |\tpeople, country, years, time, today, \n",
      " 24| -6048.46 |\tyear, Co., %, companies, years, \n",
      "  1| -6079.13 |\tPalestinians, talks, officials, Israelis, meeting, \n",
      " 16| -6270.80 |\tcourt, lawyers, prosecutors, information, case, \n",
      " 15| -6283.75 |\tgovernment, nation, rebels, officials, attempt, \n",
      " 20| -6289.97 |\tIraq, Wednesday, Sunday, United States, Middle East, \n",
      "  6| -6289.97 |\texplosion, Yemen, sailors, investigators, members, \n",
      " 14| -6298.96 |\tyear, %, period, profit, payments, \n",
      " 11| -6304.68 |\tcrew, Baghdad, boat, water, weapons, \n",
      "  3| -6307.46 |\tattack, destroyer, security, Pentagon, spokesman, \n",
      " 19| -6310.16 |\trates, %, rate, market, Inc, \n",
      " 10| -6312.80 |\tPresident Vojislav Kostunica, Belgrade, Democratic Opposition, Slobodan Milosevic, elections, \n",
      " 51| -6315.37 |\tadministration, man, sailors, Norwegians, revelations, \n",
      " 13| -6315.37 |\tthousands, rally, seats, demonstrators, police, \n",
      " 28| -6317.85 |\tday, production, demand, barrel, barrels, \n"
     ]
    }
   ],
   "source": [
    "gs.print_topics(referents_vocab,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating topicality measure q_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Media test for E_pro and E_lex: p=0.000000\n"
     ]
    }
   ],
   "source": [
    "N = len(docs)\n",
    "e_lex = np.zeros(N)\n",
    "e_pro = np.zeros(N)\n",
    "# Calc E_pro and E_lex\n",
    "for doc_index in xrange(N):\n",
    "    E_lex = expectation_ref(refs_by_doc[doc_index][\"lex\"],gs,doc_index,referents_vocab)\n",
    "    E_pro = expectation_ref(refs_by_doc[doc_index][\"pro\"],gs,doc_index,referents_vocab)\n",
    "    e_lex[doc_index] = E_lex\n",
    "    e_pro[doc_index] = E_pro\n",
    "\n",
    "# Calc qd: topicalty measure\n",
    "qd = e_pro - e_lex\n",
    "# don't consider cases were e_pro==-INF\n",
    "noinf_idx = qd!=-np.inf\n",
    "# median test\n",
    "stat,p,m,_ = scipy.stats.median_test(e_pro[noinf_idx],e_lex[noinf_idx])\n",
    "\n",
    "print(\"\\nMedia test for E_pro and E_lex: p=%f\" % p)\n",
    "\n",
    "#p(q) as histogram\n",
    "plt.figure()\n",
    "plt.hist(qd[noinf_idx],20,normed=True)\n",
    "plt.title(\"p(q)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make table  | Target | Referent | topic_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                        Target |                       Referent | Ordered topic index\n",
      "----------------------------------------------------------------------------\n",
      "   President Francesco Cossiga |                         leader | 0\n",
      "                  Ariel Sharon |                         leader | 0\n",
      "                  Palestinians |          Palestinian Authority | 2\n",
      "                          Gaza |                     Gaza Strip | 2\n",
      "                        Castle |                  Andrew Castle | 3\n",
      "             E. Robert Wallach |                        Wallach | 4\n",
      "                       Webster |           CIA Director Webster | 4\n",
      "                    US Admiral |           Admiral Robert Nader | 6\n",
      "             President Clinton |                    Mr. Clinton | 6\n",
      "                          Aden |                           port | 6\n",
      "----------------------------------------------------------------------------\n",
      "                         their |                         people | 0\n",
      "                           she |                         person | 0\n",
      "                          them |                         people | 0\n",
      "                            it |                          slump | 1\n",
      "                          they |                    delegations | 2\n",
      "                            us |                          World | 2\n",
      "                            we |                          World | 2\n",
      "                           its |          Palestinian Authority | 2\n",
      "                         their |                         judges | 3\n",
      "                           its |                          trial | 3\n",
      "----------------------------------------------------------------------------\n",
      "                 demonstrators |                         people | 0\n",
      "                    government |                          power | 0\n",
      "                           age |                          years | 1\n",
      "                   negotiators |                    delegations | 2\n",
      "                           set |                      framework | 2\n",
      "                        region |                     Gaza Strip | 2\n",
      "                      violence |                        attacks | 2\n",
      "                          land |                    territories | 2\n",
      "                         plane |                  Pan Am Flight | 3\n",
      "                          town |                      Lockerbie | 3\n",
      "----------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# make: -pos_w,k_ord_idx,tuple_id\n",
    "target_ref_k = {} # pos_w: [k_ord,tupleid]\n",
    "token_ref_id = [] # token, ref as word_ids\n",
    "\n",
    "for ( (pos_w,token),refs ) in ref_by_token.items():\n",
    "    for ref in refs:\n",
    "        if ref==token:\n",
    "            continue\n",
    "        tr_idx = tuple([vocab.index(token),vocab.index(ref)])\t\t# using global vocabulary\n",
    "        tuple_id = -1\n",
    "        if tr_idx in token_ref_id:\n",
    "            tuple_id = token_ref_id.index(tr_idx)\n",
    "        else:\n",
    "            tuple_id = len(token_ref_id)\n",
    "            token_ref_id.append(tr_idx)\n",
    "\n",
    "        ref_id = gs._vocab.index(referents_vocab.index(ref))\t\t# get referent id in HDP vocab\n",
    "        k = gs.assign_topic_word(ref_id)\n",
    "        k_ord_idx = np.nonzero(gs._ord_topics_pk[:,1]==k)[0][0]\n",
    "        if pos_w not in target_ref_k:\n",
    "            target_ref_k[pos_w] = []\n",
    "        target_ref_k[pos_w].append( (k_ord_idx,tuple_id) )\n",
    "\n",
    "for key in target_ref_k.keys():\n",
    "    target_ref_k[key].sort()\n",
    "\n",
    "## plot: target | referent | topic_id\n",
    "max_per_pos = 10\n",
    "print(\"\\n%30s | %30s | Ordered topic index\" % (\"Target\",\"Referent\"))\n",
    "print(\"----------------------------------------------------------------------------\")\n",
    "for pos,tups in target_ref_k.items():\n",
    "    for i in xrange(max_per_pos):\n",
    "        k_ord_idx ,tup_id = tups[i]\n",
    "        target = vocab[ token_ref_id[tup_id][0] ]\n",
    "        ref    = vocab[ token_ref_id[tup_id][1] ]\n",
    "        #k = gs._ord_topics_pk[k_ord_idx,1]\n",
    "\n",
    "        print(\"%30s | %30s | %i\" % (target,ref,k_ord_idx))\n",
    "    print(\"----------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Significance tests for Qd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###########\n",
    "# Media test using prob referents directly from corpus (p_{ri}=c_{ri}/c_d)\n",
    "print(\"\\nCalculating q_d for p(ri|doc_d) from corpus...\")\t\n",
    "e_lex_corpus = np.zeros(N)\n",
    "e_pro_corpus = np.zeros(N)\n",
    "# Calc E_pro and E_lex\n",
    "for doc_index in xrange(N):\n",
    "    E_lex = expectation_ref_counts(refs_by_doc[doc_index][\"lex\"],refs_by_doc[doc_index])\n",
    "    E_pro = expectation_ref_counts(refs_by_doc[doc_index][\"pro\"],refs_by_doc[doc_index])\n",
    "    e_lex_corpus[doc_index] = E_lex\n",
    "    e_pro_corpus[doc_index] = E_pro\n",
    "\n",
    "# Calc qd: topicalty measure\n",
    "qd_corpus = e_pro_corpus - e_lex_corpus\n",
    "# don't consider cases were e_pro==-INF\n",
    "noinf_idx = qd_corpus!=-np.inf\n",
    "# median test\n",
    "stat,p,m,_ = scipy.stats.median_test(e_pro_corpus[noinf_idx],e_lex_corpus[noinf_idx])\n",
    "print(\"\\nMedia test for E_pro and E_lex: p=%.4f\" % p)\n",
    "\n",
    "#p(qd_corpus) as histogram\n",
    "plt.figure()\n",
    "plt.hist(qd_corpus[noinf_idx],20,normed=True,histtype='stepfilled')\n",
    "plt.title(\"p(q')\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Logistic Regression models setup\n",
    "# get features for models 1 and 2 and export to csv for further analysis in R\n",
    "x,y = get_lr_format(refs_by_doc,gs,referents_vocab)\n",
    "output = open(\"lr_models.csv\",'w')\n",
    "output.write(\"Y,X1,X2\\n\")\n",
    "for i in xrange(len(y)):\n",
    "    output.write(\"%i,%.4f,%.4f\\n\" % (y[i],x[i][0],x[i][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
