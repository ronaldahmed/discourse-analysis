{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load utils.py\n",
    "import os,sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import glob as gb\n",
    "import re\n",
    "import ipdb\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "np.seterr(divide='ignore')\n",
    "split_re = re.compile(r'([()|])')\n",
    "###################################################################################\n",
    "\n",
    "reflexives = {\n",
    "\t'myself',\n",
    "\t'yourself',\n",
    "\t'himself',\n",
    "\t'herself',\n",
    "\t'itself',\n",
    "\t'yourselves',\n",
    "\t'themselves',\n",
    "\t'ourselves',\n",
    "}\n",
    "\n",
    "indexicals = {\t# not present in OntoNotes\n",
    "\t'that',\n",
    "\t'this',\n",
    "}\n",
    "\n",
    "dep_rels_considered = [\n",
    "\t'SBJ',\n",
    "\t'OBJ',\n",
    "\t'PMOD',\n",
    "]\n",
    "\n",
    "third_person_prp = [\n",
    "\t'he',\n",
    "\t'him',\n",
    "\t'his',\t\t# always NMOD, so never extracted\n",
    "\t'she',\n",
    "\t'her',\n",
    "\t'hers',\n",
    "\t'it',\n",
    "\t'its',\t\t# always NMOD or COORD, so never extracted\n",
    "\t'they',\n",
    "\t'them',\n",
    "\t'their',\t# always NMOD, so never extracted\n",
    "]\n",
    "\n",
    "pronounBydeprel = {\n",
    "\t'SBJ': ['he','him','she','her','hers','it','they'],\n",
    "\t'OBJ': ['him','her','it','them','they'],\n",
    "\t'PMOD': ['him','her','it','them']\n",
    "}\n",
    "\n",
    "pronoun_type_debug = [\n",
    "\t'sing_mas',\n",
    "\t'sing_fem',\n",
    "\t'sing_neu',\n",
    "\t'plural',\n",
    "\t'<NOT FOUND>',\n",
    "]\n",
    "\n",
    "pronounBytype = {\n",
    "\t'sing_mas': ['he','him','his'],\n",
    "\t'sing_fem': ['she','her','hers'],\n",
    "\t'sing_neu': ['it','its'],\n",
    "\t'plural': ['they','them','their']\n",
    "}\n",
    "\n",
    "#################################################################################################\n",
    "class Entity:\n",
    "\tdef __init__(self,_phrase='',_pos='-',global_pos=-np.inf):\n",
    "\t\tself.phrase = _phrase\n",
    "\t\tself.pos = _pos\n",
    "\t\tself.global_pos = global_pos\n",
    "\t\tself.entity_id = -1\n",
    "\t\tself.referent_id = -1 # global id in referents vocabulary\n",
    "\t\tself.dep_rel = \"\"\n",
    "\n",
    "\t\t\n",
    "\tdef __str__(self):\n",
    "\t\tgp = self.global_pos\n",
    "\t\tif gp==-np.inf:\n",
    "\t\t\tgp=-1\n",
    "\t\tres = \"[ phrase:%s |  pos: %s |  global_pos: %i |  entity_id: %i  | dep_rel: %s ]\" % (self.phrase,self.pos,gp,self.entity_id,self.dep_rel)\n",
    "\t\treturn res\n",
    "\n",
    "\tdef __repr__(self):\n",
    "\t\tgp = self.global_pos\n",
    "\t\tif gp==-np.inf:\n",
    "\t\t\tgp=-1\n",
    "\t\tres = \"[ phrase:%s |  pos: %s |  global_pos: %i |  entity_id: %i  | dep_rel: %s ]\" % (self.phrase,self.pos,gp,self.entity_id,self.dep_rel)\n",
    "\t\treturn res\n",
    "\n",
    "\"\"\"\n",
    "Read entities, considering maximally spanning NPs (ignoring nested NPs and nested entities).\n",
    "Entity selection is done with constraints in NNP and PRP - see paper -.\n",
    "\n",
    "\"\"\"\n",
    "def read_conll2010_task1_max_np(filename,n_docs='all'):\n",
    "\tdocuments = []\n",
    "\tsentence = []\n",
    "\tcount_docs = 0\n",
    "\tnesting_depth = np.inf\n",
    "\tglobal_line = 0\n",
    "\n",
    "\tfor line in open(filename):\n",
    "\t\tline = line.strip('\\n')\n",
    "\t\tif line.startswith('#begin'):\n",
    "\t\t\tdoc = []\n",
    "\t\t\tentity_filter = np.zeros(1000,dtype=bool)\t# {ent_id: True, if there is at least one NNP in the chain}\n",
    "\t\t\tchain_lens = np.zeros(1000)\t# {ent_id: count of elements in the chain}\n",
    "\n",
    "\t\t\tnesting_depth = np.inf\n",
    "\t\t\tevent = Entity()\n",
    "\t\t\tglobal_line = 0\n",
    "\t\t\tcontinue\n",
    "\t\t\t#all temp files are reset\n",
    "\t\tif line.startswith('#end'):\n",
    "\t\t\tdocuments.append(doc)\n",
    "\t\t\tcount_docs+=1\n",
    "\t\t\tif n_docs!='all':\n",
    "\t\t\t\tif count_docs>=n_docs:\n",
    "\t\t\t\t\tbreak\n",
    "\t\t\tcontinue\n",
    "\t\t\n",
    "\t\tif line=='':\n",
    "\t\t\tnesting_depth = np.inf\t# default val=INF\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\t## MAIN STUFF\n",
    "\t\tglobal_line += 1 \t\t# increment in each valid line\n",
    "\t\tcomp = line.split('\\t')\n",
    "\t\ttoken = comp[1].lower()\t\t# consider lowercase for all\n",
    "\t\tpos = comp[4]\n",
    "\t\tdep_rel = comp[10]\n",
    "\t\tcoref_str = comp[-1]\n",
    "\n",
    "\t\t\"\"\"\n",
    "\t\t# only consecutive proper nouns\n",
    "\t\tif nesting_depth!=np.inf:\n",
    "\t\t\tif pos in ['NNP','NNPS'] and event.pos==pos:\n",
    "\t\t\t\tevent.phrase += \" \"+token\n",
    "\t\t\telse:\n",
    "\t\t\t\tevent.pos = '-'\n",
    "\t\t\"\"\"\n",
    "\t\t\n",
    "\t\t# maximally spanning NPs considered, labeled as NNP is there is an NNP in the phrase\n",
    "\t\tif nesting_depth!=np.inf:\t# only enters for maximally entities\n",
    "\t\t\tevent.phrase += \" \"+token\n",
    "\t\t\tif pos in ['NNP','NNPS']:\n",
    "\t\t\t\tevent.pos = pos\n",
    "\t\t\telif event.pos not in ['NNP','NNPS']:\n",
    "\t\t\t\tevent.pos = '-'\n",
    "\n",
    "\t\t\tif dep_rel=='SBJ' and event.dep_rel!='SBJ':\n",
    "\t\t\t\tevent.dep_rel = dep_rel\n",
    "\t\t\telif dep_rel=='OBJ' and event.dep_rel!='SBJ':\n",
    "\t\t\t\tevent.dep_rel = dep_rel\n",
    "\t\t\telif dep_rel=='PMOD' and event.dep_rel not in ['SBJ','OBJ','NAME']:\n",
    "\t\t\t\tevent.dep_rel = dep_rel\n",
    "\n",
    "\t\tif coref_str=='_':\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\ttemp = split_re.split(coref_str)\n",
    "\t\tsplitted = [a for a in temp if a!='']\n",
    "\t\tk=0\n",
    "\n",
    "\t\twhile(k<len(splitted)):\t\t# | no hace nada\n",
    "\t\t\tif splitted[k]=='(':\n",
    "\t\t\t\tif nesting_depth==np.inf:\n",
    "\t\t\t\t\tnesting_depth=0\n",
    "\t\t\t\t\tevent = Entity(token,pos,global_line)\t# initialization of active entity\n",
    "\t\t\t\t\tevent.dep_rel = dep_rel\n",
    "\t\t\t\tnesting_depth+=1\n",
    "\t\t\telif splitted[k].isdigit():\n",
    "\t\t\t\tid = int(splitted[k])\n",
    "\t\t\t\tif nesting_depth==1:\t# depth of 1 corresponds to maximal entity\n",
    "\t\t\t\t\tevent.entity_id = id\n",
    "\t\t\telif splitted[k]==')':\n",
    "\t\t\t\tnesting_depth-=1\n",
    "\t\t\t\tif nesting_depth==0: \t# if ) is closing maximal\n",
    "\t\t\t\t\t# cleaning phrase\n",
    "\t\t\t\t\tphrase = event.phrase.strip(' ')\n",
    "\t\t\t\t\tif len(phrase)>1:\n",
    "\t\t\t\t\t\tif phrase[-2:]==' .' or phrase[-2:]==' ,':\t# if ends in point or comma\n",
    "\t\t\t\t\t\t\tphrase = phrase[:-2]\n",
    "\t\t\t\t\t\tphrase = phrase.strip(' ')\t# get rid of spaces on sides\n",
    "\t\t\t\t\tevent.phrase = phrase\n",
    "\n",
    "\t\t\t\t\tif any([ event.pos in ['NNP','NNPS'], \n",
    "\t\t\t\t\t\t\t all([\tevent.pos in ['PRP','PRP$'],\t\t# all constraints in selection of pronouns\n",
    "\t\t\t\t\t\t\t \t\ttoken in third_person_prp,\t\t# third person pronouns\n",
    "\t\t\t\t\t\t\t \t\ttoken not in reflexives,\t\t# no reflexives\n",
    "\t\t\t\t\t\t\t \t\ttoken not in indexicals,\t\t# no indexicals\n",
    "\t\t\t\t\t\t\t \t\tevent.dep_rel in dep_rels_considered,\t# pronouns in SUBJ, OBJ or PMOD position\n",
    "\t\t\t\t\t\t\t \t])\n",
    "\t\t\t\t\t\t]):\n",
    "\t\t\t\t\t\t# filtering preprocessing\n",
    "\t\t\t\t\t\tif event.pos in ['NNP','NNPS']:\t\t\t\t# found an NNP in the chain\n",
    "\t\t\t\t\t\t\tentity_filter[event.entity_id] = True\n",
    "\t\t\t\t\t\t\tif event.dep_rel not in dep_rels_considered:\n",
    "\t\t\t\t\t\t\t\tevent.dep_rel = 'SBJ'\n",
    "\t\t\t\t\t\tdoc.append(event)\n",
    "\n",
    "\t\t\t\t\tnesting_depth=np.inf\n",
    "\t\t\tk+=1\n",
    "\t#END-FOR-FILE\n",
    "\n",
    "\tdocuments = np.array(documents)\n",
    "\t\n",
    "\treturn documents\n",
    "\n",
    "\"\"\"\n",
    "Filter entities according to the following rules:\n",
    " - at least one NNP in chain\n",
    " - if one NNP in chain is present in the lexicon, consider all NNPs of chain\n",
    "\"\"\"\n",
    "def filter_entities(documents,lexicon):\n",
    "\tfiltered_docs = []\n",
    "\tfor doc in documents:\n",
    "\t\t# all elements in chains with at least one valid NNP will be considered\n",
    "\t\tvalid_chains = set()\n",
    "\t\tfor entity in doc:\n",
    "\t\t\tif any([ entity.pos=='NNPS',\t\t\t\t\t\t\t\t#plural goes in anyway\n",
    "\t\t\t\t\t entity.pos=='NNP' and entity.phrase in lexicon,\t# only if present in lexicon vocabulary\n",
    "\t\t\t\t\t]):\n",
    "\t\t\t\tvalid_chains.add(entity.entity_id)\n",
    "\t\t# filter documents\n",
    "\t\tnew_doc = np.array( [entity for entity in doc if entity.entity_id in valid_chains ] )\n",
    "\t\tfiltered_docs.append(new_doc)\n",
    "\n",
    "\tfiltered_docs = np.array(filtered_docs)\n",
    "\treturn filtered_docs\t\n",
    "\n",
    "\n",
    "#################################################################################################\n",
    "\"\"\"\n",
    "Read NP lexicon with genre and number counts. Only considered the cases: (\\w )+, (! \\w), (\\w !)\n",
    "return:\n",
    "\tcounts: (V,4) : [masculine_count  femenine_count  neutral_count  plural_count]\n",
    "\tnouns_vocab (V,) : vocabulary of NP\n",
    "\"\"\"\n",
    "def read_noun_lexicon(_dir):\n",
    "\tnouns_vocab = []\n",
    "\tcounts = []\n",
    "\tfor filename in gb.glob(_dir+'*'):\n",
    "\t\tfor line in open(filename,encoding='latin-1'):\n",
    "\t\t\tline = line.strip(\"\\n\")\n",
    "\t\t\tlast_line=line\n",
    "\t\t\tif line=='':\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tsplitted = line.split(\"\\t\")\n",
    "\t\t\tphrase = splitted[0]\n",
    "\n",
    "\t\t\tmas,fem,neu,plu = [int(a) for a in splitted[1].split(\" \")]\n",
    "\t\t\tsp_np = phrase.split(' ')\n",
    "\n",
    "\t\t\tsave = False\n",
    "\t\t\tif all([word.isalpha() for word in sp_np]):\n",
    "\t\t\t\tsave = True\n",
    "\t\t\t\"\"\"\n",
    "\t\t\t# save <! word> or <word !>\n",
    "\t\t\tif len(sp_np)>1:\n",
    "\t\t\t\tif any([\n",
    "\t\t\t\t\t len(sp_np)>1 and (sp_np[0]=='!' and sp_np[1].isalpha()),\n",
    "\t\t\t\t\t len(sp_np)>1 and (sp_np[1]=='!' and sp_np[0].isalpha()),\n",
    "\t\t\t\t]):\n",
    "\t\t\t\t\tsave = True\n",
    "\t\t\t\"\"\"\n",
    "\t\t\tif save:\n",
    "\t\t\t\tcounts.append([mas,fem,neu,plu])\n",
    "\t\t\t\tnouns_vocab.append(phrase)\n",
    "\tcounts = np.array(counts)\n",
    "\tnouns_vocab = np.array(nouns_vocab)\n",
    "\treturn counts,nouns_vocab\n",
    "\n",
    "\"\"\"\n",
    "Get intersection between corpus vocabulary and genre_number lexicon vocabulary\n",
    "\"\"\"\n",
    "def get_vocab_from_lexicon(documents,lexicon_vocab):\n",
    "\tvocab = set()\n",
    "\tfor doc in documents:\n",
    "\t\tfor ent in doc:\n",
    "\t\t\tif ent.phrase in lexicon_vocab:\n",
    "\t\t\t\tvocab.add(ent.phrase)\n",
    "\treturn vocab\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Annotate each phrase in vocab with genre and number (pronoun type id)\n",
    "\"\"\"\n",
    "def annotate_pro_type(lex_counts,lex_vocab,corpus_vocab):\n",
    "\tpro_type_ids = []\n",
    "\tfor phrase in corpus_vocab:\n",
    "\t\tpro_id=-1\n",
    "\t\tif phrase in lex_vocab:\n",
    "\t\t\tp_idx = np.nonzero(lex_vocab==phrase)[0][0]\n",
    "\t\t\tpro_id = lex_counts[p_idx,:].argmax()\n",
    "\t\telif phrase in third_person_prp:\n",
    "\t\t\tfor _type,pros in pronounBytype.items():\n",
    "\t\t\t\tif phrase in pros:\n",
    "\t\t\t\t\tpro_id = pronoun_type_debug.index(_type)\n",
    "\t\t\t\t\tbreak\n",
    "\t\telse:\n",
    "\t\t\tpro_id = pronoun_type_debug.index('plural')\t\t# if not in lexicon, PLURAL, since all NNPs are considered\n",
    "\t\tpro_type_ids.append(pro_id)\n",
    "\tpro_type_ids = np.array(pro_type_ids)\n",
    "\treturn pro_type_ids\n",
    "\n",
    "def saveObject(obj, name='model'):\n",
    "\twith open(name + '.pickle', 'wb') as fd:\n",
    "\t\tpickle.dump(obj, fd, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def uploadObject(obj_name):\n",
    "\t# Load tagger\n",
    "\twith open(obj_name + '.pickle', 'rb') as fd:\n",
    "\t\tobj = pickle.load(fd)\n",
    "\treturn obj\n",
    "\n",
    "#################################################################################################\n",
    "\"\"\"\n",
    "Get orreference chains from a single document\n",
    "@param doc: list of Entity objects\n",
    "\"\"\"\n",
    "def get_coref_chains(doc):\n",
    "\tchains = {}\n",
    "\tfor entity in doc:\n",
    "\t\tent_id = entity.entity_id\n",
    "\t\tif ent_id not in chains:\n",
    "\t\t\tchains[ent_id] = []\n",
    "\t\tpair = tuple([entity.phrase,entity.pos])\n",
    "\t\tif pair not in chains[ent_id]:\n",
    "\t\t\tchains[ent_id].append(pair)\n",
    "\t\n",
    "\treturn chains\n",
    "\n",
    "def get_referents(doc,lexicon):\n",
    "\treferents = {}\t# 1000 assumed to be max ent_id\n",
    "\tchains = get_coref_chains(doc)\n",
    "\n",
    "\tfor chain_id,chain in chains.items():\n",
    "\t\tfor ref_exp in chain:\n",
    "\t\t\ttext = ref_exp[0]\n",
    "\t\t\tpos = ref_exp[1]\n",
    "\t\t\tif pos=='NNP' and text in lexicon:\n",
    "\t\t\t\treferents[chain_id] = text\n",
    "\t\t\telif pos=='NNPS':\n",
    "\t\t\t\treferents[chain_id] = text\n",
    "\t\t\t\tbreak\n",
    "\treturn referents\n",
    "\n",
    "'''\n",
    "Reformat corpus and get vocabulary of refering expressions (pronouns and referents).\n",
    "return: new_documents: [[entity_obj with phrase:ref_exp_id, and entity_id]]\n",
    "\t\tref_exp_vocab: [all referents + all pronouns (third person PRP)]  -> assumption: a proper name only refers to itself\n",
    "'''\n",
    "def reformat_data(documents,lexicon):\n",
    "\tref_exp_vocab=[]\n",
    "\t# make referents vocabulary and coreference chains\n",
    "\tfor doc in documents:\n",
    "\t\treferents = get_referents(doc,lexicon)\n",
    "\t\tfor entity in doc:\n",
    "\t\t\treferent = referents[entity.entity_id]\n",
    "\t\t\tref_id=-1\n",
    "\t\t\tif referent in ref_exp_vocab:\n",
    "\t\t\t\tref_id = ref_exp_vocab.index(referent)\n",
    "\t\t\telse:\n",
    "\t\t\t\tref_id = len(ref_exp_vocab)\n",
    "\t\t\t\tref_exp_vocab.append(referent)\n",
    "\t\t\tentity.referent_id = ref_id\n",
    "\t\t\tif entity.pos[0]=='N':\t\t\t# replace original noun with referent (assumption: proper names only refer to themselves)\n",
    "\t\t\t\tentity.phrase = referent\n",
    "\t# add pronouns\n",
    "\tfor pro in third_person_prp:\n",
    "\t\tif pro not in ref_exp_vocab:\n",
    "\t\t\tref_exp_vocab.append(pro)\n",
    "\n",
    "\t# reformat documents\n",
    "\tnew_documents = []\n",
    "\tfor doc in documents:\n",
    "\t\tnew_doc = copy.deepcopy(doc)\n",
    "\t\tfor event in new_doc:\n",
    "\t\t\tevent.phrase = ref_exp_vocab.index(event.phrase)\n",
    "\t\tnew_documents.append(new_doc)\n",
    "\tnew_documents = np.array(new_documents)\n",
    "\tref_exp_vocab = np.array(ref_exp_vocab)\n",
    "\n",
    "\treturn new_documents,ref_exp_vocab\n",
    "\n",
    "\n",
    "def debug_dep_rel_pronoun(data):\n",
    "\tdebug = {\n",
    "\t'SBJ': set(),\n",
    "\t'OBJ': set(),\n",
    "\t'PMOD': set()\n",
    "\t}\n",
    "\tfor doc in data:\n",
    "\t\tfor ent in doc:\n",
    "\t\t\tif ent.pos[0]=='P':\n",
    "\t\t\t\tdebug[ent.dep_rel].add(ent.phrase)\n",
    "\treturn debug\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load class_speaker_model.py\n",
    "import numpy as np\n",
    "import scipy\n",
    "from utils import  third_person_prp, pronoun_type_debug, pronounBydeprel, pronounBytype\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import ipdb\n",
    "\n",
    "class SpeakerModel:\n",
    "\tdef __init__(self,_data,_vocab,_pronoun_type_ids,_lex_counts,_alpha=0.1,_decay=3, _salience='recency'):\n",
    "\t\t# corpus  Ndocs x entities\n",
    "\t\tself.data = _data\n",
    "\t\t# vocab: list of phrases, used to map phr_id in corpus\n",
    "\t\tself.vocab = _vocab # np array\n",
    "\t\tself.vocab_size = len(_vocab)\n",
    "\t\t# number of referents\n",
    "\t\tself._n_referents = self.vocab_size - len(third_person_prp)\n",
    "\t\t# pronoun type (e.g. singular_male) ids in same order as vocabulary\n",
    "\t\tself.pronoun_type_ids = _pronoun_type_ids\n",
    "\t\t# number of pronoun types\n",
    "\t\tself._n_pro_type = len(pronoun_type_debug) - 1 # don't count NOT FOUND\n",
    "\t\t# counts of unseen referents by pronoun types, got from lexicon\n",
    "\t\tself._u_pro_type = np.zeros(self._n_pro_type)\n",
    "\t\t# total number of unseen referents\n",
    "\t\tself._u_total = 0\n",
    "\n",
    "\t\t# parameter for prob of new referent\n",
    "\t\tself._alpha = _alpha\n",
    "\t\t# decay for discourse salience\n",
    "\t\tself._decay = _decay\n",
    "\t\t# discourse salience measure, p(r)| vals: ['freq','rec']\n",
    "\t\tself._salience = _salience\n",
    "\n",
    "\t\t# number of pronouns and proper names to predict\n",
    "\t\tself._n_samples = sum([len(doc) for doc in self.data])\n",
    "\t\t# V: number of ref. expressions that can refer to r, constant across referents\n",
    "\t\tself._V = self._n_samples / self._n_referents\n",
    "\t\t# word likelihood given referent p(w|r)\n",
    "\t\tself._p_w_r = 1.0 / self._V\n",
    "\n",
    "\t\t#class labels\n",
    "\t\tself.class_labels = ['PRP','NNP']\n",
    "\t\t# true class\n",
    "\t\tself._Y \t = []\n",
    "\t\t# predicted class\n",
    "\t\tself._Y_pred = []\n",
    "\n",
    "\t\t#calc_unseen_counts lex_counts\n",
    "\t\tself.calc_unseen_counts(_lex_counts)\n",
    "\n",
    "\t\t# save metrics\n",
    "\t\tself._model_loglikelihood = 0.0\n",
    "\t\tself._total_acc = 0.0\n",
    "\t\tself._np_acc = 0.0\t\t# proper name accuracy\n",
    "\t\tself._pro_acc = 0.0\t\t# pronoun accuracy\n",
    "\n",
    "\n",
    "\tdef calc_unseen_counts(self,_lex_counts):\n",
    "\t\tself._u_pro_type = _lex_counts[:self._n_pro_type]\n",
    "\t\tself._u_total = _lex_counts[-1]\n",
    "\n",
    "\tdef speech_cost(self,word_id):\n",
    "\t\t#return len( self.vocab[word_id] )\n",
    "\t\treturn np.log( len(self.vocab[word_id]) )\n",
    "\t\t#return np.log( len(self.vocab[word_id]) )+1.0\n",
    "\n",
    "\t'''\n",
    "\tp(r) : discourse salience of referent r up until now\n",
    "\t'''\n",
    "\tdef get_salience(self,referent_id,ref_prev_mentions,last_mention_dist):\n",
    "\t\tp_r = 0\n",
    "\t\tif ref_prev_mentions==0:\t\t\t# new referent\n",
    "\t\t\tpro_type = self.pronoun_type_ids[referent_id]\n",
    "\t\t\tp_r = self._alpha * self._u_pro_type[pro_type] / self._u_total\n",
    "\t\telse:\n",
    "\t\t\tif self._salience=='frequency':\n",
    "\t\t\t\tp_r = ref_prev_mentions\t\t\t\t\t\t# frequency measure\n",
    "\t\t\telse:\t\t\t\t\t\t\t\t\t\t\t# 'rec'\n",
    "\t\t\t\tp_r = np.exp(-last_mention_dist/self._decay)\t# recency measure\n",
    "\t\treturn p_r\n",
    "\n",
    "\t'''\n",
    "\tSum over all potencial referents compatible with w. Sum_{r'} {p(w|r')*p(r')}\n",
    "\t'''\n",
    "\tdef get_sum_potencial_referents(self,pos,referent_id,counts_state,mention_state,global_pos):\n",
    "\t\t# proper noun only refers to itself\n",
    "\t\tif pos[0]=='N':\n",
    "\t\t\tlast_mention_dist = global_pos - mention_state[referent_id]\n",
    "\t\t\tprev_mentions = counts_state[referent_id]\n",
    "\t\t\treturn self.get_salience(referent_id,prev_mentions,last_mention_dist)\n",
    "\t\t# pronoun spotted\n",
    "\t\telse:\n",
    "\t\t\tpro_type = self.pronoun_type_ids[referent_id]\n",
    "\t\t\t# get potencial referents ids in vocabulary\n",
    "\t\t\tcond = self.pronoun_type_ids==pro_type\n",
    "\t\t\tpotencial_refs_ids = []\n",
    "\t\t\tfor i,val in enumerate(cond):\n",
    "\t\t\t\tif val and self.vocab[i] not in third_person_prp:\n",
    "\t\t\t\t\tpotencial_refs_ids.append(i)\n",
    "\n",
    "\t\t\t# sum over all potencial referents\n",
    "\t\t\tsum_p_r = 0\n",
    "\t\t\tfor ref_id in potencial_refs_ids:\n",
    "\t\t\t\tlast_mention_dist = global_pos - mention_state[ref_id]\n",
    "\t\t\t\tprev_mentions = counts_state[ref_id]\n",
    "\t\t\t\tif prev_mentions!=0:\t\t\t\t\t\t\t\t\t\t\t\t# sum only active referents\n",
    "\t\t\t\t\tsum_p_r +=  self.get_salience(ref_id,prev_mentions,last_mention_dist)\n",
    "\n",
    "\t\t\tif sum_p_r==0:\t\t# if PRP and there is no referent mentioned before\n",
    "\t\t\t\treturn 0.0\t\t# return \n",
    "\t\t\tsum_p_r += self.get_salience(referent_id,0,0)\t\t# add unseen entity prob\n",
    "\n",
    "\t\t\treturn sum_p_r\n",
    "\n",
    "\t\"\"\"\n",
    "\tPredict between pronoun or proper name for each referent as discourse advances.\n",
    "\t\"\"\"\n",
    "\tdef predict(self):\n",
    "\t\tself._model_loglikelihood = 0.0\t\t# calculate model log likelihood as it predicts\n",
    "\t\tfor doc in self.data:\n",
    "\t\t\treferent_counts = np.zeros(1000)\n",
    "\t\t\tlast_mention = np.zeros(1000)\n",
    "\t\t\tfor entity in doc:\n",
    "\t\t\t\tdep_rel = entity.dep_rel\n",
    "\t\t\t\tref_id = entity.referent_id\n",
    "\t\t\t\tpro_type_name = pronoun_type_debug[ self.pronoun_type_ids[ref_id] ]\n",
    "\n",
    "\t\t\t\ttrue_pos = entity.pos[:3]\n",
    "\t\t\t\t# true label\n",
    "\t\t\t\tself._Y.append(self.class_labels.index(true_pos))\n",
    "\t\t\t\t# predicting refering expression (POS)\n",
    "\t\t\t\tpred_pos = self.class_labels.index('NNP')\t# pick proper name by default\n",
    "\n",
    "\t\t\t\t#print(\"ref_exp:\",self.vocab[entity.phrase])\n",
    "\t\t\t\t#print(\"referent:\",self.vocab[ref_id])\n",
    "\n",
    "\t\t\t\t# discourse salience for referent r\n",
    "\t\t\t\tlast_mention_distance = entity.global_pos - last_mention[ref_id]\n",
    "\t\t\t\tp_r = self.get_salience(ref_id,referent_counts[ref_id],last_mention_distance)\n",
    "\n",
    "\t\t\t\t# PROPER NAME CASE\n",
    "\t\t\t\tsum_p_r_np = self.get_sum_potencial_referents('NNP', ref_id, referent_counts, last_mention, entity.global_pos)\n",
    "\t\t\t\tcw_np = self.speech_cost(ref_id)\n",
    "\t\t\t\tlog_speaker_np =  -np.log(sum_p_r_np) - np.log(cw_np)\n",
    "\n",
    "\t\t\t\tif abs(log_speaker_np)!=np.inf:\n",
    "\t\t\t\t\tself._model_loglikelihood += log_speaker_np\n",
    "\n",
    "\t\t\t\t#ipdb.set_trace()\n",
    "\n",
    "\t\t\t\t# PRONOUN CASE\n",
    "\t\t\t\tsum_p_r_pro = self.get_sum_potencial_referents('PRP', ref_id, referent_counts, last_mention, entity.global_pos)\n",
    "\t\t\t\t# speaker's cost: cross agreement and grammatical position\n",
    "\t\t\t\tlog_speaker_pro = -np.inf\n",
    "\t\t\t\tfor pro in pronounBydeprel[dep_rel]:\n",
    "\t\t\t\t\tif pro in pronounBytype[pro_type_name]:\n",
    "\t\t\t\t\t\t# valid pronouns\n",
    "\t\t\t\t\t\tpro_id = np.nonzero(self.vocab==pro)[0][0]\n",
    "\t\t\t\t\t\tcw_pro = self.speech_cost(pro_id)\n",
    "\n",
    "\t\t\t\t\t\tlocal_log_speaker_pro = 0.0\n",
    "\t\t\t\t\t\tif sum_p_r_pro==0:\t\t# no referent active\n",
    "\t\t\t\t\t\t\tlocal_log_speaker_pro = -np.inf\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tlocal_log_speaker_pro = -np.log(sum_p_r_pro) - np.log(cw_pro)\n",
    "\t\t\t\t\t\tlog_speaker_pro = max(log_speaker_pro,local_log_speaker_pro)\n",
    "\n",
    "\t\t\t\t\t\t# add model log likelihood only if Ps != 0\n",
    "\t\t\t\t\t\tif local_log_speaker_pro!=-np.inf:\n",
    "\t\t\t\t\t\t\tself._model_loglikelihood += local_log_speaker_pro\n",
    "\n",
    "\t\t\t\tif log_speaker_pro > log_speaker_np:\n",
    "\t\t\t\t\tpred_pos = self.class_labels.index('PRP')\n",
    "\n",
    "\t\t\t\t# predicted label\n",
    "\t\t\t\tself._Y_pred.append(pred_pos)\n",
    "\n",
    "\t\t\t\t#update counts\n",
    "\t\t\t\treferent_counts[ref_id] += 1\n",
    "\t\t\t\tlast_mention[ref_id] = entity.global_pos\n",
    "\n",
    "\t\t\t\t#print(\"Predicted label:\",self.class_labels[pred_pos])\n",
    "\t\t\t\t#ipdb.set_trace()\n",
    "\t\t\t#END-FOR-DOC\n",
    "\t\t#END-FOR-DOCUMENTS\n",
    "\t\tself._Y = np.array(self._Y)\n",
    "\t\tself._Y_pred = np.array(self._Y_pred)\n",
    "\n",
    "\n",
    "\tdef evaluate(self,verbose=True):\n",
    "\t\tclass_accs = []\n",
    "\t\tfor i in range(len(self.class_labels)):\n",
    "\t\t\tlabel = self.class_labels[i]\n",
    "\t\t\tacc_class = sum((self._Y==i) * (self._Y_pred==i)) / sum(self._Y==i)\n",
    "\t\t\tif label[0]=='P':\n",
    "\t\t\t\tself._pro_acc = acc_class\n",
    "\t\t\telse:\n",
    "\t\t\t\tself._np_acc  = acc_class\n",
    "\t\t\tclass_accs.append(acc_class)\n",
    "\t\t\tif verbose:\n",
    "\t\t\t\tprint(\"Accuracy %s: %.2f\" % (label,acc_class*100))\n",
    "\t\tself._total_acc = accuracy_score(self._Y,self._Y_pred)\n",
    "\t\tif verbose:\n",
    "\t\t\tprint(\"Total accuracy: %.2f\" % (self._total_acc*100) )\n",
    "\t\t\tprint(\"Model log likelihood: %.2f\" % self._model_loglikelihood)\n",
    "\t\tif 0 in class_accs:\n",
    "\t\t\treturn 0\n",
    "\t\treturn self._total_acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load alternative_models.py\n",
    "from class_speaker_model import SpeakerModel\n",
    "import numpy as np\n",
    "\n",
    "class SM_NoDiscourse(SpeakerModel):\n",
    "\t'''\n",
    "\tp(r) : Uniform discourse salience\n",
    "\t'''\n",
    "\tdef get_salience(self,referent_id,ref_prev_mentions,last_mention_dist):\n",
    "\t\treturn 1.0 / self._n_referents\n",
    "\n",
    "class SM_NoCost(SpeakerModel):\n",
    "\tdef speech_cost(self,word_id):\n",
    "\t\treturn 1\n",
    "\n",
    "class SM_NoUnseen(SpeakerModel):\n",
    "\tdef calc_unseen_counts(self,_lex_counts):\n",
    "\t\tself._u_pro_type = np.ones(self._n_pro_type)\n",
    "\t\tself._u_total = 1\t\t#cte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_conll_dir = \"../datasets/semeval_2010_t1_eng/data\"\n",
    "data_noun_lexicon_dir = \"../datasets/noun_gender_number/\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data and resources reading configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_docs='all'\n",
    "update_pron_types = False\t\t# True: load agreement annotation for referents\n",
    "update_lexicon = False\t\t\t# True: load vocabulary intersection between genre_number lexicon and corpus\n",
    "\n",
    "GN_counts = []\n",
    "GN_np_vocab = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_file = os.path.join(data_conll_dir,'en.train.txt')\n",
    "docs = read_conll2010_task1_max_np(data_file,n_docs=n_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read proper names present in genre_number lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gn_read = False\n",
    "if update_lexicon:\n",
    "    print(\"Reading genre_number lexicon...\")\n",
    "    GN_counts, GN_np_vocab = read_noun_lexicon(data_noun_lexicon_dir)\t# 1.9 GB of memory OMG\n",
    "    gn_read = True\n",
    "    lexicon = get_vocab_from_lexicon(docs,GN_np_vocab)\n",
    "    saveObject(lexicon,'names_in_gnlexicon')\n",
    "else:\n",
    "    lexicon = uploadObject('names_in_gnlexicon')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data filtering and formating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = filter_entities(docs,lexicon)\n",
    "formated_data,vocab = reformat_data(docs,lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of pronouns and proper names in corpus, after filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pronouns:  251\n",
      "Proper nouns:  1044\n",
      "Ref_vocab:  458\n"
     ]
    }
   ],
   "source": [
    "pros = 0\n",
    "nps = 0\n",
    "for doc in docs:\n",
    "    nps  += sum([1 for ent in doc if ent.pos[0]=='N'])\n",
    "    pros += sum([1 for ent in doc if ent.pos[0]=='P'])\n",
    "\n",
    "print(\"Pronouns: \",pros)\n",
    "print(\"Proper nouns: \",nps)\n",
    "print(\"Ref_vocab: \",len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annotate agreement information for referents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pro_type_ids = []\n",
    "if update_pron_types:\n",
    "    if not gn_read:\n",
    "        print(\"Reading genre_number lexicon...\")\n",
    "        GN_counts, GN_np_vocab = read_noun_lexicon(data_noun_lexicon_dir)\t# 1.9 GB of memory OMG\n",
    "    pro_type_ids = annotate_pro_type(GN_counts,GN_np_vocab,vocab)\n",
    "\n",
    "    n_pro_types = len(pronoun_type_debug)-1\n",
    "    pro_type_counts = [sum(GN_counts[:,i]!=0) for i in range(n_pro_types)]\n",
    "    pro_type_counts.append( GN_counts.shape[0] )\n",
    "\n",
    "    saveObject(pro_type_ids,'pro_type_ids')\n",
    "    saveObject(pro_type_counts,'pro_type_counts')\n",
    "else:\n",
    "    pro_type_ids = uploadObject('pro_type_ids')\n",
    "    pro_type_counts = uploadObject('pro_type_counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                             god :        sing_mas\n",
      "                                                                     mike jensen :        sing_mas\n",
      "                                                                         richard :        sing_mas\n",
      "----------------------------------------------------------------------------------------------------\n",
      "                                                                           betsy :        sing_fem\n",
      "----------------------------------------------------------------------------------------------------\n",
      "                                                                         alabama :        sing_neu\n",
      "                                                                   sunday school :        sing_neu\n",
      "                                                                american medical :        sing_neu\n",
      "                                                                 social security :        sing_neu\n",
      "                                                                        nbc news :        sing_neu\n",
      "                                                                        new york :        sing_neu\n",
      "                                                                          israel :        sing_neu\n",
      "                                                                        tel aviv :        sing_neu\n",
      "                                                                       jerusalem :        sing_neu\n",
      "                                                                    the israelis :        sing_neu\n",
      "----------------------------------------------------------------------------------------------------\n",
      "                        members of both parties , both republicans and democrats :          plural\n",
      "                          the nonpartisan center on budget and policy priorities :          plural\n",
      "                                                          deaths of palestinians :          plural\n",
      "                                                  a number of other palestinians :          plural\n",
      "                                                                    palestinians :          plural\n",
      "                                                                the palestinians :          plural\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Check agreement information for first 20 phrases in vocabulary\n",
    "for ii in range(len(pronoun_type_debug)-1):\n",
    "    for i in range(20):\n",
    "        if pro_type_ids[i]==ii:\n",
    "            print(\"%80s : %15s\" % (vocab[i],pronoun_type_debug[pro_type_ids[i]] ) )\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune parameters for speaker model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RECENCY: Optimum parameters:------\n",
      "   alpha: 0.0001\n",
      "   decay: 100\n",
      "Training with optimum parameters...\n",
      "Accuracy PRP: 86.85\n",
      "Accuracy NNP: 73.18\n",
      "Total accuracy: 75.83\n",
      "Model log likelihood: 7303.38\n",
      "-------------------------------------------------------\n",
      "FREQUENCY: Optimum parameters:------\n",
      "   alpha: 0.0001\n",
      "   decay: 100\n",
      "Training with optimum parameters...\n",
      "Accuracy PRP: 79.28\n",
      "Accuracy NNP: 71.93\n",
      "Total accuracy: 73.36\n",
      "Model log likelihood: 3883.20\n",
      "-------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#Finding optimum values of alpha and decay\n",
    "alphas = [1e-4, 1e-3, 1e-2, 0.1,1,10]\n",
    "decays = [1e-2,0.1, 1, 3, 5, 10,100,1e3]\n",
    "salience_measures = ['recency','frequency']\n",
    "\n",
    "a_opt, d_opt = 0,0\n",
    "acc = 0\n",
    "\n",
    "for salience in salience_measures:\n",
    "    for alpha in alphas:\n",
    "        for decay in decays:\n",
    "            spk = SpeakerModel(\t_data=formated_data,\n",
    "                                _vocab=vocab,\n",
    "                                _pronoun_type_ids=pro_type_ids,\n",
    "                                _lex_counts=pro_type_counts,\n",
    "                                _alpha=alpha,\n",
    "                                _decay=decay,\n",
    "                                _salience=salience)\n",
    "            spk.predict()\n",
    "            total_acc = spk.evaluate(verbose=False)\n",
    "            if total_acc>acc:\n",
    "                acc = total_acc\n",
    "                a_opt=alpha\n",
    "                d_opt=decay\n",
    "        #END-FOR-DECAY\n",
    "    #END-FOR-ALPHA\n",
    "    print(\"%s: Optimum parameters:------\" % salience.upper())\n",
    "    print(\"   alpha:\",a_opt)\n",
    "    print(\"   decay:\",d_opt)\n",
    "    print(\"Training with optimum parameters...\")\n",
    "    spk = SpeakerModel(\t_data=formated_data,\n",
    "                        _vocab=vocab,\n",
    "                        _pronoun_type_ids=pro_type_ids,\n",
    "                        _lex_counts=pro_type_counts,\n",
    "                        _alpha=a_opt,\n",
    "                        _decay=d_opt,\n",
    "                        _salience=salience)\n",
    "    spk.predict()\n",
    "    spk.evaluate()\n",
    "    print(\"-------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run speaker models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running models...\n",
      "  alpha:  0.0001\n",
      "  decay:  100\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "       Model |       Discourse || T_ACC | Pronoun_ACC | ProperName_ACC | Log-lhood |\n",
      "====================================================================================================\n",
      "    complete |         recency || 75.83 |       86.85 |          73.18 |   7303.38 |\n",
      "    complete |       frequency || 73.36 |       79.28 |          71.93 |   3883.20 |\n",
      "----------------------------------------------------------------------------------------------------\n",
      "  -discourse |              NA || 66.49 |       39.04 |          73.08 |  14199.09 |\n",
      "----------------------------------------------------------------------------------------------------\n",
      "       -cost |         recency || 80.62 |        0.00 |         100.00 |   8091.49 |\n",
      "       -cost |       frequency || 80.62 |        0.00 |         100.00 |   4671.30 |\n",
      "----------------------------------------------------------------------------------------------------\n",
      "     -unseen |         recency || 75.75 |       86.85 |          73.08 |   6434.60 |\n",
      "     -unseen |       frequency || 73.36 |       79.28 |          71.93 |   3018.80 |\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#alpha = 1e-4\n",
    "#decay = 100\n",
    "alpha = a_opt\n",
    "decay = d_opt\n",
    "salience_measures = ['recency','frequency']\n",
    "print(\"Running models...\")\n",
    "print(\"  alpha: \",alpha)\n",
    "print(\"  decay: \",decay)\n",
    "print(\"-\"*150)\n",
    "\n",
    "print(\"%12s | %15s || %5s | %11s | %14s | %9s |\" % (\"Model\",\"Discourse\",\"T_ACC\",\"Pronoun_ACC\",\"ProperName_ACC\",\"Log-lhood\") )\n",
    "print(\"=\"*100)\n",
    "# Complete model\n",
    "for salience in salience_measures:\n",
    "    spk = SpeakerModel( _data=formated_data,\n",
    "                        _vocab=vocab,\n",
    "                        _pronoun_type_ids=pro_type_ids,\n",
    "                        _lex_counts=pro_type_counts,\n",
    "                        _alpha=alpha,\n",
    "                        _decay=decay,\n",
    "                        _salience=salience)\n",
    "    spk.predict()\n",
    "    spk.evaluate(verbose=False)\n",
    "    print(\"%12s | %15s || %2.2f | %11.2f | %14.2f | %9.2f |\" % \n",
    "            (\"complete\", salience, spk._total_acc*100, spk._pro_acc*100, spk._np_acc*100, spk._model_loglikelihood) )\n",
    "print(\"-\"*100)\n",
    "\n",
    "# No discourse model\n",
    "spk = SM_NoDiscourse(\t_data=formated_data,\n",
    "                        _vocab=vocab,\n",
    "                        _pronoun_type_ids=pro_type_ids,\n",
    "                        _lex_counts=pro_type_counts,\n",
    "                        _alpha=alpha,\n",
    "                        _decay=decay,\n",
    "                        _salience=salience)\n",
    "spk.predict()\n",
    "spk.evaluate(verbose=False)\n",
    "print(\"%12s | %15s || %2.2f | %11.2f | %14.2f | %9.2f |\" % \n",
    "        (\"-discourse\", \"NA\", spk._total_acc*100, spk._pro_acc*100, spk._np_acc*100, spk._model_loglikelihood) )\n",
    "print(\"-\"*100)\n",
    "\n",
    "# No cost model\n",
    "for salience in salience_measures:\n",
    "    spk = SM_NoCost( _data=formated_data,\n",
    "                    _vocab=vocab,\n",
    "                    _pronoun_type_ids=pro_type_ids,\n",
    "                    _lex_counts=pro_type_counts,\n",
    "                    _alpha=alpha,\n",
    "                    _decay=decay,\n",
    "                    _salience=salience)\n",
    "    spk.predict()\n",
    "    spk.evaluate(verbose=False)\n",
    "    print(\"%12s | %15s || %2.2f | %11.2f | %14.2f | %9.2f |\" % \n",
    "            (\"-cost\", salience, spk._total_acc*100, spk._pro_acc*100, spk._np_acc*100, spk._model_loglikelihood) )\n",
    "print(\"-\"*100)\n",
    "\n",
    "# No estimates of unseen referents\n",
    "for salience in salience_measures:\n",
    "    spk = SM_NoUnseen(  _data=formated_data,\n",
    "                        _vocab=vocab,\n",
    "                        _pronoun_type_ids=pro_type_ids,\n",
    "                        _lex_counts=pro_type_counts,\n",
    "                        _alpha=alpha,\n",
    "                        _decay=decay,\n",
    "                        _salience=salience)\n",
    "    spk.predict()\n",
    "    spk.evaluate(verbose=False)\n",
    "    print(\"%12s | %15s || %2.2f | %11.2f | %14.2f | %9.2f |\" % \n",
    "            (\"-unseen\", salience, spk._total_acc*100, spk._pro_acc*100, spk._np_acc*100, spk._model_loglikelihood) )\n",
    "print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
